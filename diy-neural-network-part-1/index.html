<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>DIY neural network (part 1)</title>
    <link rel="stylesheet" href="../assets/built/screen.css%3Fv=a00d03b799.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700,700i%7CDosis:600,700&amp;subset=latin-ext">
        
    <link rel="canonical" href="index.html" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    <link rel="amphtml" href="amp/index.html" />
    
    <meta property="og:site_name" content="sffresch" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="DIY neural network (part 1)" />
    <meta property="og:description" content="We are going to start as simple as it can get: Creating a baby neural network that can barely solve problems each of us could easily solve by hand." />
    <meta property="og:url" content="http://frank690.github.io/diy-neural-network-part-1/" />
    <meta property="og:image" content="https://images.unsplash.com/photo-1505235687559-28b5f54645b7?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDF8fHNlZWRsaW5nfGVufDB8fHx8MTY0MTIzODkzNg&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000" />
    <meta property="article:published_time" content="2021-08-17T18:28:45.000Z" />
    <meta property="article:modified_time" content="2022-05-14T17:16:05.000Z" />
    <meta property="article:tag" content="ANN" />
    <meta property="article:tag" content="data-science" />
    <meta property="article:tag" content="DIY" />
    <meta property="article:tag" content="neural-network" />
    <meta property="article:tag" content="python" />
    
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="DIY neural network (part 1)" />
    <meta name="twitter:description" content="We are going to start as simple as it can get: Creating a baby neural network that can barely solve problems each of us could easily solve by hand." />
    <meta name="twitter:url" content="http://frank690.github.io/diy-neural-network-part-1/" />
    <meta name="twitter:image" content="https://images.unsplash.com/photo-1505235687559-28b5f54645b7?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDF8fHNlZWRsaW5nfGVufDB8fHx8MTY0MTIzODkzNg&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Frank Eschner" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="ANN, data-science, DIY, neural-network, python" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="1124" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "sffresch",
        "url": "http://frank690.github.io/",
        "logo": {
            "@type": "ImageObject",
            "url": "http://frank690.github.io/favicon.ico",
            "width": 48,
            "height": 48
        }
    },
    "author": {
        "@type": "Person",
        "name": "Frank Eschner",
        "image": {
            "@type": "ImageObject",
            "url": "http://frank690.github.io/content/images/2021/08/68586209_10205828296182785_2060227135364136960_n.jpg",
            "width": 1536,
            "height": 2048
        },
        "url": "http://frank690.github.io/author/frank690/",
        "sameAs": [
            "https://sffresch.de"
        ]
    },
    "headline": "DIY neural network (part 1)",
    "url": "http://frank690.github.io/diy-neural-network-part-1/",
    "datePublished": "2021-08-17T18:28:45.000Z",
    "dateModified": "2022-05-14T17:16:05.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://images.unsplash.com/photo-1505235687559-28b5f54645b7?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fHNlZWRsaW5nfGVufDB8fHx8MTY0MTIzODkzNg&ixlib=rb-1.2.1&q=80&w=2000",
        "width": 2000,
        "height": 1124
    },
    "keywords": "ANN, data-science, DIY, neural-network, python",
    "description": "We are going to start as simple as it can get: Creating a baby neural network that can barely solve problems each of us could easily solve by hand.",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://frank690.github.io/"
    }
}
    </script>

    <meta name="generator" content="Ghost 4.17" />
    <link rel="alternate" type="application/rss+xml" title="sffresch" href="../rss/index.html" />
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-KGHY4JLHRR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-KGHY4JLHRR');
</script>

<!-- Inline Latex Math -->
<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']],
     processEscapes: true}          
   });
</script>

<!-- Latex Math -->
<script type="text/javascript" async
src="https://cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!--- copy to clipboard -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism-tomorrow.min.css" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/plugins/toolbar/prism-toolbar.min.css" />

<!--- code styling -->
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.6.0/themes/prism-okaidia.min.css" rel="stylesheet" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.6.0/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet" />

<!--- cookie consent -->
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.css" /><style>:root {--ghost-accent-color: #650179;}</style>
</head>

<body class="post-template tag-ann tag-data-science tag-diy tag-neural-network tag-python">
<div id='particles-js'></div>
    <div class="site">
        <header class="site-header">
    <div class="navbar">
        <div class="navbar-left">
            <a class="logo" href="../index.html">
        <span class="logo-text">sffresch</span>
</a>        </div>
            <nav class="main-menu hidden-xs hidden-sm hidden-md">
                <ul class="nav-list u-plain-list">
        <li
            class="menu-item menu-item-whoami">
            <a class="menu-item-link" href="../whoami/index.html">whoami</a>
        </li>
        <li
            class="menu-item menu-item-projects">
            <a class="menu-item-link" href="../projects.html">projects</a>
        </li>
</ul>
            </nav>
        <div class="navbar-right">
            <div class="social hidden-xs hidden-sm"></div>
            <div class="burger hidden-lg hidden-xl">
    <div class="burger-bar"></div>
    <div class="burger-bar"></div>
</div>        </div>
    </div>
</header>        <div class="site-content">
            
<div class="container">
    <div class="row">
        <div class="content-column col-sm-12">
            <div class="content-area">
                <main class="site-main">
                        <article
                            class="post tag-ann tag-data-science tag-diy tag-neural-network tag-python single u-shadow">
                            <div class="post-meta">
                                <time class="post-meta-date"
                                    datetime="2021-08-17">
                                    Aug 17, 2021
                                </time>
                                <span
                                    class="post-meta-length">7 min read</span>
                            </div>
                                <figure class="post-media">
        <div class="u-placeholder">
            <a class="post-image-link" href="index.html">
            <div class="on-media-title no-select">
                DIY neural network (part 1)
            </div>
            <img class="post-image no-select lazyload"
                data-srcset="https://images.unsplash.com/photo-1505235687559-28b5f54645b7?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDF8fHNlZWRsaW5nfGVufDB8fHx8MTY0MTIzODkzNg&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000 400w, https://images.unsplash.com/photo-1505235687559-28b5f54645b7?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDF8fHNlZWRsaW5nfGVufDB8fHx8MTY0MTIzODkzNg&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000 750w, https://images.unsplash.com/photo-1505235687559-28b5f54645b7?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDF8fHNlZWRsaW5nfGVufDB8fHx8MTY0MTIzODkzNg&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000 960w"
                src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="
                data-sizes="auto" alt="DIY neural network (part 1)">
            </a>
        </div>

                <figcaption>Photo by <a href="https://unsplash.com/@sushobhan?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit">Sushobhan Badhai</a> / <a href="https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit">Unsplash</a></figcaption>
    </figure>
                            <div class="post-wrapper">
                                <div class="post-content u-text-format">
                                    <p>This is the first post of a series about creating your very own artificial neural network (ANN) by hand. We are going to start as simple as it can get: Creating a baby neural network that can barely solve problems each of us could easily solve by hand. In each part of this series, we will continue to improve the capabilities of our self-made ANN and try to tackle more challenging problems with it.</p><p><strong>TL;DR</strong>: <a href="https://github.com/frank690/sffresch-code/blob/main/diy-neural-network-part-1/code.ipynb">Take me to the code</a>.</p><h3 id="the-problem">The Problem</h3><p>Consider having three light bulbs. The first two of them (A and B) can be turned either ON (1) or OFF (0) <em>independently</em>. The state of the third light bulb (C) depends on the first two (A and B). Try to figure out how the underlying systematic works by clicking on A and/or B as much as you like.</p><!--kg-card-begin: html--><div class="lightbulbs">
    <div class="lightbulb">
        <img id="lightbulb_A" class="input_lightbulb" state="off" onclick="input_lightbulb_clicked(this.id)" src="../content/images/2021/08/light-bulb-off.png" alt="A"></img>
    	<span class="caption">A</span>
	</div>
	<div class="lightbulb">
        <img id="lightbulb_B" class="input_lightbulb" state="off" onclick="input_lightbulb_clicked(this.id)" src="../content/images/2021/08/light-bulb-off.png" alt="B"></img>
    	<span class="caption">B</span>
	</div>
	<div class="lightbulb">
        <img id="target_lightbulb" src="../content/images/2021/08/light-bulb-on.png" alt="C"></img>
    	<span class="caption">C</span>
	</div>
</div><!--kg-card-end: html--><p>The presented problem is known as the <a href="https://en.wikipedia.org/wiki/XNOR_gate">XNOR gate</a>. If A and B are both either 0 or 1, C will be 1. Otherwise, C will be 0. The corresponding input data (the states of A and B) will be stored in the variable X and the output data (the state of C) as y.</p><h3 id="generating-more-data">Generating more data</h3><p><s>Unfortunately,</s> Luckily, the data we got is a bit noisy on the input side and we do not get a clean 0 or 1 for the states of A and B, but rather a value around (!) 0 or 1. Since it is very unlikely that two samples are going to be the very same, we can now easily draw way more data than the initial four possible states (of A and B).</p><pre><code class="language-python">import numpy as np
from typing import Tuple, Dict

def generate_data(N: int) -&gt; Tuple[np.ndarray, np.ndarray]:
    """
    Original code credits to Prof. Dr. Stefan Harmeling
    
    Generate a data for training our linear model.
    :param N: number of samples multiplier.
    :return: tuple of x and y data as numpy ndarrays.
    """
    X = np.repeat(np.array([[0, 0], [0, 1], [1, 0], [1, 1]]), N, axis=0)
    X = X + np.random.randn(4 * N, 2) * 0.2
    y = np.repeat([0, 1, 1, 0], N)
    y = np.reshape(y, (len(y), 1))

    return X, y</code></pre><p>Using the generate_data function gets us some nice training and testing sets to feed to our ANN.</p><pre><code class="language-python">X_train, y_train = generate_data(N=100)
X_test, y_test = generate_data(N=50)</code></pre><figure class="kg-card kg-image-card"><img src="../content/images/2022/01/xnor_problem-4.svg" class="kg-image" alt loading="lazy"></figure><h3 id="initialize-weights-and-biases">Initialize weights and biases</h3><p>Since we now know what our data looks like, let us define a simple ANN with one layer each for the input, hidden, and output layers. The input layer will have two neurons (corresponding to the input data A and B) and the output layer will have one neuron (C). We choose the number of hidden neurons to be 3.</p><p>Side note: <br>There is no general answer to the question of choosing how many hidden layers and neurons per (hidden) layer one should use. In my personal experience, this highly depends on multiple factors. To name just a few, one is of course the complexity of the problem that the ANN should try to solve. Another one is the computing time one can spend and the capabilities of the machine that trains the ANN. Generally speaking, one should stick with <a href="https://en.wikipedia.org/wiki/Occam's_razor">Occam's razor</a>: If you have two similarly performing models, choose the simpler one.</p><pre><code class="language-python">weights = dict()
biases = dict()

weights[1] = 2 * np.random.random((2,3)) - 1
biases[1] = np.zeros(3)

weights[2] = 2 * np.random.random((3,1)) - 1
biases[2] = np.zeros(1)</code></pre><h3 id="forward-propagation">Forward propagation</h3><p>Given some input data, compute the resulting prediction (hypothesis). We introduce non-linearity to our ANN by using the well-known <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid function</a> (and its derivative).</p><pre><code class="language-python">def sigmoid(X: np.ndarray) -&gt; np.ndarray:
    """
    The sigmoid function will return 0 for every x &lt;&lt; 0 and 1 for every x &gt;&gt; 0.
    :param X: data to transform via sigmoid function:
    :return: transformed data that lies between 0 and 1.
    """
    return 1 / (1 + np.exp(-X))</code></pre><pre><code class="language-python">def sigmoid_derivative(X: np.ndarray) -&gt; np.ndarray:
    """
    The derivative of the sigmoid function.
    :param X: the data points for that the slope of the sigmoid function should be returned.
    :return: the slope of the sigmoid function at every given X.
    """
    sig = sigmoid(X)
    return sig * (1 - sig)</code></pre><p>Side note:<br>Instead of computing the resulting values for one sample at a time (via for-loops), I prefer to use the vectorized version of the same computations due to cleaner code and faster computations.</p><pre><code class="language-python">z = dict()
a = dict()

z[1] = X @ weights[1] + biases[1]
a[1] = sigmoid(z[1])

z[2] = a[1] @ weights[2] + biases[2]
a[2] = sigmoid(z[2])

h = a[2]</code></pre><h3 id="computing-the-loss">Computing the loss</h3><p>Since we are trying to solve a binary classification problem (predicting if the result is either 0 or 1), we will use the <a href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression">cross-entropy loss function</a>.</p><pre><code class="language-python">def cross_entropy_loss(
    h: np.ndarray, y: np.ndarray
) -&gt; np.ndarray:
    """
    Compute the cross entropy loss for the given hypothesis (h) in contrast to the true results (y).
    :param h: Hypothesis of the NN to compare with y.
    :param y: True results of the data.
    :return: Cost/Loss of the current hypothesis.
    """
    return -(1/y.size) * ((y.T @ np.log(h)) + ((1 - y.T) @ np.log(1 - h)))</code></pre><pre><code class="language-python">def cross_entropy_derivative(
    h: np.ndarray, y: np.ndarray
) -&gt; np.ndarray:
    """
    Compute the derivative of the cross entropy loss.
    :param h: Hypothesis of the NN to compare with y.
    :param y: True results of the data.
    :return: Derivative of the cross entropy loss with the current hypothesis.
    """
    return -(y // h) + ((1 - y) // (1 - h))</code></pre><h3 id="backpropagation">Backpropagation</h3><p>After computing the prediction of our neural network (also known as the hypothesis), we now work our way back(wards) to the weights and biases. This is done by looking at the loss and computing the gradients of each of our previously computed values.</p><pre><code class="language-python">dz = dict()
da = dict()

da[2] = cross_entropy_derivative(h=h, y=y)
dz[2] = da[2] * sigmoid_derivative(z[2])

da[1] = dz[2] @ weights[2].T
dz[1] = da[1] * sigmoid_derivative(z[1])</code></pre><h3 id="gradient-descent">Gradient descent</h3><p>To update our weights and biases so that the next prediction of our neural network will be a bit better than the previous one, we use <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>. For simplicity, let us choose a learning rate of 1 (and thus omit it in the code).</p><pre><code class="language-python">weights[1] -= (X.T @ dz[1]) * (1 / y.size)
biases[1] -= np.mean(dz[1], axis=0)

weights[2] -= (a[1].T @ dz[2]) * (1 / y.size)
biases[2] -= np.mean(dz[2], axis=0)</code></pre><h3 id="putting-it-all-together">Putting it all together</h3><p>Since we are descending one step at a time towards a minimum (due to the way how gradient descent works), we repeat the process of forward- and backpropagation through the network a couple of times (here, 1000 times). Putting this information plus every previous step into a single function called fit, our ANN is ready to go.</p><pre><code class="language-python">def fit(X: np.ndarray, y: np.ndarray) -&gt; Tuple[Dict, Dict]:
    """
    Create a simple ANN and train it on the given binary classification data.
    :param X: Input data.
    :param y: Output data.
    :return: Weights and biases as dictionaries.
    """
    weights = dict()
    biases = dict()

    weights[1] = 2 * np.random.random((2,3)) - 1
    biases[1] = np.zeros(3)
    weights[2] = 2 * np.random.random((3,1)) - 1
    biases[2] = np.zeros(1)

    a = dict()
    z = dict()
    da = dict()
    dz = dict()

    for iteration in range(1000):
        z[1] = X @ weights[1] + biases[1]
        a[1] = sigmoid(z[1])
        z[2] = a[1] @ weights[2] + biases[2]
        a[2] = sigmoid(z[2])
        h = a[2]

        loss = cross_entropy_loss(h=h, y=y)
        print(f"Loss ({iteration}): {loss[0][0]}")

        da[2] = cross_entropy_derivative(h=h, y=y)
        dz[2] = da[2] * sigmoid_derivative(z[2])
        da[1] = dz[2] @ weights[2].T
        dz[1] = da[1] * sigmoid_derivative(z[1])

        weights[1] -= (X.T @ dz[1]) * (1 / y.size)
        biases[1] -= np.mean(dz[1], axis=0)

        weights[2] -= (a[1].T @ dz[2]) * (1 / y.size)
        biases[2] -= np.mean(dz[2], axis=0)

    return weights, biases</code></pre><h3 id="training">Training</h3><p>Using our previously created training set, we can run the fit function and train our ANN. In return, we will get the fitted weights and biases that represent the <em>brain</em> of our ANN.</p><pre><code class="language-python">w, b = fit(X=X_train, y=y_train)</code></pre><pre><code class="language-shell">Loss (0): 0.701788446507631
Loss (1): 0.6932204170602805
Loss (2): 0.6926989143616228
Loss (3): 0.6925255938589576
...
Loss (997): 0.08240692560029927
Loss (998): 0.08234153724021534
Loss (999): 0.08227351128655165</code></pre><h3 id="predicting">Predicting</h3><p>To predict the output of data our ANN has not seen before, we need a dedicated predict function that takes the fitted weights and biases as input parameters.</p><pre><code class="language-python">def predict(w: Dict, b: Dict, X: np.ndarray) -&gt; np.ndarray:
    """
    Use the given weights (w) and biases (b) to make a prediction for the given input data (X).
    :param w: Dictionary of weight matrices.
    :param b: Dictionary of bias vectors.
    :param X: Input data to make predictions on.
    :return: An numpy array of predictions for every sample in X.
    """
    z = dict()
    a = dict()
    
    z[1] = X @ w[1] + b[1]
    a[1] = sigmoid(z[1])
    z[2] = a[1] @ w[2] + b[2]
    a[2] = sigmoid(z[2])
    
    return a[2]</code></pre><h3 id="results">Results</h3><p>Looking at the course of the loss, we can see that our ANN was able to fit the given data in such a way, that the resulting loss (for the training data) decreased significantly. Plotting the testing dataset together with the decision boundaries, one can see that our ANN successfully learned to classify data of the XNOR problem (nearly always) correctly. </p><figure class="kg-card kg-image-card"><img src="../content/images/2022/01/xnor_decision_boundaries-2.svg" class="kg-image" alt loading="lazy" width="1920" height="1440"></figure><h3 id="what-is-next">What is next?</h3><p>Now having a baby ANN at our fingertips, there are plenty of things to implement and improve to solve more challenging problems. This includes introducing <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">stochastic gradient descent</a>, different kinds of regularization methods, <a href="https://golden.com/wiki/Nesterov_momentum">Nesterov momentum</a>, <a href="https://arxiv.org/abs/1502.01852">more sophisticated weight + bias initialization</a>, <a href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html">more loss functions</a>, <a href="https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html">more activation functions</a>, etc. But besides the mathematical perspective, there is also a lot of room for improving the code quality.</p><h3 id="acronyms">Acronyms</h3><p>ANN - Artificial Neural Network<br></p>
                                </div>
                                    <h2 class="post-tags-header">Tags</h2>
    <div class="post-tags">
            <a class="post-tag" id="ANN" href="../tag/ann/index.html" style="background-color: #8e0101;">ANN</a>
            <a class="post-tag" id="data-science" href="../tag/data-science/index.html" style="background-color: #366356;">data-science</a>
            <a class="post-tag" id="DIY" href="../tag/diy/index.html" style="background-color: #a87d34;">DIY</a>
            <a class="post-tag" id="neural-network" href="../tag/neural-network/index.html" style="background-color: #600094;">neural-network</a>
            <a class="post-tag" id="python" href="../tag/python/index.html" style="background-color: #026600;">python</a>
    </div>
                            </div>
                        </article>
                </main>
            </div>
        </div>
    </div>
</div>
        </div>
        <footer class="site-footer">
    <div class="container">
        <div class="footer-inner">
            <div class="social">
                <a class="social-item social-item-rss"
                    href="https://feedly.com/i/subscription/feed/http://frank690.github.io/rss/"
                    target="_blank" rel="noopener noreferrer" aria-label="RSS">
                    <i class="icon icon-rss"></i>
                </a>
            </div>
            <div class="copyright">
                Powered by <a href="https://ghost.org/" target="_blank">Ghost</a>
            </div>
        </div>
    </div>
</footer>    </div>


    <div class="dimmer"></div>
<div class="off-canvas">
    <div class="burger burger-close hidden-lg hidden-xl">
    <div class="burger-bar"></div>
    <div class="burger-bar"></div>
</div>    <div class="mobile-menu">
        <ul class="nav-list u-plain-list">
        <li
            class="menu-item menu-item-whoami">
            <a class="menu-item-link" href="../whoami/index.html">whoami</a>
        </li>
        <li
            class="menu-item menu-item-projects">
            <a class="menu-item-link" href="../projects.html">projects</a>
        </li>
</ul>
    </div>
    <aside class="widget-area">
    <div class="widget widget-facebook widget-no-title u-shadow">
    <div class="fb-page" data-href="__YOUR_FACEBOOK_PAGE_URL__"
        data-small-header="false" data-hide-cover="false"
        data-show-facepile="true" data-hide-cta="false" data-tabs="none">
    </div>
</div>    <div class="widget widget-tags">
    <h4 class="widget-title">Tags</h4>
        <div class="tag-feed">
                <a class="post-tag" id="ANN" href="../tag/ann/index.html" style="background-color: #8e0101;">ANN</a>
                <a class="post-tag" id="DIY" href="../tag/diy/index.html" style="background-color: #a87d34;">DIY</a>
                <a class="post-tag" id="classification" href="../tag/classification/index.html" style="background-color: #517510;">classification</a>
                <a class="post-tag" id="data-science" href="../tag/data-science/index.html" style="background-color: #366356;">data-science</a>
                <a class="post-tag" id="log-loss" href="../tag/log-loss/index.html" style="background-color: #ff2424;">log-loss</a>
                <a class="post-tag" id="machine-learning" href="../tag/machine-learning/index.html" style="background-color: #001999;">machine-learning</a>
                <a class="post-tag" id="neural-network" href="../tag/neural-network/index.html" style="background-color: #600094;">neural-network</a>
                <a class="post-tag" id="python" href="../tag/python/index.html" style="background-color: #026600;">python</a>
                <a class="post-tag" id="softmax" href="../tag/softmax/index.html" style="background-color: #1c226d;">softmax</a>
                <a class="post-tag" id="theory" href="../tag/theory/index.html" style="background-color: #bf7e0d;">theory</a>
        </div>
</div></aside></div>
    <script src="https://code.jquery.com/jquery-3.3.1.min.js"
        integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
        crossorigin="anonymous">
        </script>
    <script src="../assets/built/main.min.js%3Fv=a00d03b799"></script>

    <div id="fb-root"></div>
<script async defer crossorigin="anonymous"
    src="https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v3.2"></script>
    

    <!-- copy to clipboard -->
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/prism.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/plugins/toolbar/prism-toolbar.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.6.0/components/prism-python.min.js" ></script>
<script src="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.js" data-cfasync="false"></script>
<script>
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#ffffff"
    },
    "button": {
      "background": "#8ec760",
      "text": "#ffffff"
    }
  },
  "theme": "edgeless"
});
</script>
</body>

</html>
<!DOCTYPE html>
<html ⚡>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <title>DIY neural network (part 1)</title>

    <link rel="canonical" href="../index.html" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:site_name" content="sffresch" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="DIY neural network (part 1)" />
    <meta property="og:description" content="We are going to start as simple as it can get: Creating a baby neural network that can barely solve problems each of us could easily solve by hand." />
    <meta property="og:url" content="http://frank690.github.io/diy-neural-network-part-1/" />
    <meta property="og:image" content="https://images.unsplash.com/photo-1505235687559-28b5f54645b7?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDF8fHNlZWRsaW5nfGVufDB8fHx8MTY0MTIzODkzNg&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000" />
    <meta property="article:published_time" content="2021-08-17T18:28:45.000Z" />
    <meta property="article:modified_time" content="2022-05-14T17:16:05.000Z" />
    <meta property="article:tag" content="ANN" />
    <meta property="article:tag" content="data-science" />
    <meta property="article:tag" content="DIY" />
    <meta property="article:tag" content="neural-network" />
    <meta property="article:tag" content="python" />
    
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="DIY neural network (part 1)" />
    <meta name="twitter:description" content="We are going to start as simple as it can get: Creating a baby neural network that can barely solve problems each of us could easily solve by hand." />
    <meta name="twitter:url" content="http://frank690.github.io/diy-neural-network-part-1/" />
    <meta name="twitter:image" content="https://images.unsplash.com/photo-1505235687559-28b5f54645b7?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDF8fHNlZWRsaW5nfGVufDB8fHx8MTY0MTIzODkzNg&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Frank Eschner" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="ANN, data-science, DIY, neural-network, python" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="1124" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "sffresch",
        "url": "http://frank690.github.io/",
        "logo": {
            "@type": "ImageObject",
            "url": "http://frank690.github.io/favicon.ico",
            "width": 48,
            "height": 48
        }
    },
    "author": {
        "@type": "Person",
        "name": "Frank Eschner",
        "image": {
            "@type": "ImageObject",
            "url": "http://frank690.github.io/content/images/2021/08/68586209_10205828296182785_2060227135364136960_n.jpg",
            "width": 1536,
            "height": 2048
        },
        "url": "http://frank690.github.io/author/frank690/",
        "sameAs": [
            "https://sffresch.de"
        ]
    },
    "headline": "DIY neural network (part 1)",
    "url": "http://frank690.github.io/diy-neural-network-part-1/",
    "datePublished": "2021-08-17T18:28:45.000Z",
    "dateModified": "2022-05-14T17:16:05.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://images.unsplash.com/photo-1505235687559-28b5f54645b7?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDF8fHNlZWRsaW5nfGVufDB8fHx8MTY0MTIzODkzNg&ixlib=rb-1.2.1&q=80&w=2000",
        "width": 2000,
        "height": 1124
    },
    "keywords": "ANN, data-science, DIY, neural-network, python",
    "description": "We are going to start as simple as it can get: Creating a baby neural network that can barely solve problems each of us could easily solve by hand.",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://frank690.github.io/"
    }
}
    </script>

    <meta name="generator" content="Ghost 4.17" />
    <link rel="alternate" type="application/rss+xml" title="sffresch" href="../../rss/index.html" />

    <style amp-custom>
    *,
    *::before,
    *::after {
        box-sizing: border-box;
    }

    html {
        overflow-x: hidden;
        overflow-y: scroll;
        font-size: 62.5%;
        -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
    }

    body {
        min-height: 100vh;
        margin: 0;
        padding: 0;
        color: #3a4145;
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        font-size: 1.7rem;
        line-height: 1.55em;
        font-weight: 400;
        font-style: normal;
        background: #fff;
        scroll-behavior: smooth;
        overflow-x: hidden;
        -webkit-font-smoothing: antialiased;
        -moz-osx-font-smoothing: grayscale;
    }

    p,
    ul,
    ol,
    li,
    dl,
    dd,
    hr,
    pre,
    form,
    table,
    video,
    figure,
    figcaption,
    blockquote {
        margin: 0;
        padding: 0;
    }

    ul[class],
    ol[class] {
        padding: 0;
        list-style: none;
    }

    img {
        display: block;
        max-width: 100%;
    }

    input,
    button,
    select,
    textarea {
        font: inherit;
        -webkit-appearance: none;
    }

    fieldset {
        margin: 0;
        padding: 0;
        border: 0;
    }

    label {
        display: block;
        font-size: 0.9em;
        font-weight: 700;
    }

    hr {
        position: relative;
        display: block;
        width: 100%;
        height: 1px;
        border: 0;
        border-top: 1px solid currentcolor;
        opacity: 0.1;
    }

    ::selection {
        text-shadow: none;
        background: #cbeafb;
    }

    mark {
        background-color: #fdffb6;
    }

    small {
        font-size: 80%;
    }

    sub,
    sup {
        position: relative;
        font-size: 75%;
        line-height: 0;
        vertical-align: baseline;
    }
    sup {
        top: -0.5em;
    }
    sub {
        bottom: -0.25em;
    }

    ul li + li {
        margin-top: 0.6em;
    }

    a {
        color: var(--ghost-accent-color, #1292EE);
        text-decoration-skip-ink: auto;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
        margin: 0;
        font-weight: 700;
        color: #121212;
        line-height: 1.4em;
    }

    h1 {
        font-size: 3.4rem;
        line-height: 1.1em;
    }

    h2 {
        font-size: 2.4rem;
        line-height: 1.2em;
    }

    h3 {
        font-size: 1.8rem;
    }

    h4 {
        font-size: 1.7rem;
    }

    h5 {
        font-size: 1.6rem;
    }

    h6 {
        font-size: 1.6rem;
    }

    amp-img {
        height: 100%;
        width: 100%;
        max-width: 100%;
        max-height: 100%;
    }

    amp-img img {
        object-fit: cover;
    }

    .page-header {
        padding: 50px 5vmin 30px;
        text-align: center;
        font-size: 2rem;
        text-transform: uppercase;
        letter-spacing: 0.5px;
    }

    .page-header a {
        color: #121212;
        font-weight: 700;
        text-decoration: none;
        font-size: 1.6rem;
        letter-spacing: -0.1px;
    }

    .post {
        max-width: 680px;
        margin: 0 auto;
    }

    .post-header {
        margin: 0 5vmin 5vmin;
        text-align: center;
    }

    .post-meta {
        margin: 1rem 0 0 0;
        text-transform: uppercase;
        color: #738a94;
        font-weight: 500;
        font-size: 1.3rem;
    }

    .post-image {
        margin: 0 0 5vmin;
    }

    .post-image img {
        display: block;
        width: 100%;
        height: auto;
    }

    .post-content {
        padding: 0 5vmin;
    }

    .post-content > * + * {
        margin-top: 1.5em;
    }

    .post-content [id]:not(:first-child) {
        margin: 2em 0 0;
    }

    .post-content > [id] + * {
        margin-top: 1rem;
    }

    .post-content [id] + .kg-card,
    .post-content blockquote + .kg-card {
        margin-top: 40px;
    }

    .post-content > ul,
    .post-content > ol,
    .post-content > dl {
        padding-left: 1.9em;
    }

    .post-content hr {
        margin-top: 40px;
    }

    .post .post-content hr + * {
        margin-top: 40px;
    }

    .post-content amp-img {
        background-color: #f8f8f8;
    }

    .post-content blockquote {
        position: relative;
        font-style: italic;
    }

    .post-content blockquote::before {
        content: "";
        position: absolute;
        left: -1.5em;
        top: 0;
        bottom: 0;
        width: 0.3rem;
        background: var(--ghost-accent-color, #1292EE);
    }

    .post-content :not(.kg-card):not([id]) + .kg-card {
        margin-top: 40px;
    }

    .post-content .kg-card + :not(.kg-card) {
        margin-top: 40px;
    }

    .kg-card figcaption {
        padding: 1.5rem 1.5rem 0;
        text-align: center;
        font-weight: 500;
        font-size: 1.3rem;
        line-height: 1.4em;
        opacity: 0.6;
    }

    .kg-card figcaption strong {
        color: rgba(0,0,0,0.8);
    }

    .post-content :not(pre) code {
        vertical-align: middle;
        padding: 0.15em 0.4em 0.15em;
        border: #e1eaef 1px solid;
        font-weight: 400;
        font-size: 0.9em;
        line-height: 1em;
        color: #15171a;
        background: #f0f6f9;
        border-radius: 0.25em;
    }

    .post-content > pre {
        overflow: scroll;
        padding: 16px 20px;
        color: #fff;
        background: #1F2428;
        border-radius: 5px;
        box-shadow: 0 2px 6px -2px rgba(0,0,0,.1), 0 0 1px rgba(0,0,0,.4);
    }

    .kg-embed-card {
        display: flex;
        flex-direction: column;
        align-items: center;
        width: 100%;
    }

    .kg-image-card img {
        margin: auto;
    }

    .kg-gallery-card + .kg-gallery-card {
        margin-top: 0.75em;
    }

    .kg-gallery-container {
        position: relative;
    }

    .kg-gallery-row {
        display: flex;
        flex-direction: row;
        justify-content: center;
    }

    .kg-gallery-image {
        width: 100%;
        height: 100%;
    }

    .kg-gallery-row:not(:first-of-type) {
        margin: 0.75em 0 0 0;
    }

    .kg-gallery-image:not(:first-of-type) {
        margin: 0 0 0 0.75em;
    }

    .kg-bookmark-card,
    .kg-bookmark-publisher {
        position: relative;
    }

    .kg-bookmark-container,
    .kg-bookmark-container:hover {
        display: flex;
        flex-wrap: wrap;
        flex-direction: row-reverse;
        color: currentColor;
        background: rgba(255,255,255,0.6);
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        text-decoration: none;
        border-radius: 5px;
        box-shadow: 0 2px 6px -2px rgba(0, 0, 0, 0.1), 0 0 1px rgba(0, 0, 0, 0.4);
        overflow: hidden;
    }

    .kg-bookmark-content {
        flex-basis: 0;
        flex-grow: 999;
        padding: 20px;
        order: 1;
    }

    .kg-bookmark-title {
        font-weight: 600;
        font-size: 1.5rem;
        line-height: 1.3em;
    }

    .kg-bookmark-description {
        display: -webkit-box;
        max-height: 45px;
        margin: 0.5em 0 0 0;
        font-size: 1.4rem;
        line-height: 1.55em;
        overflow: hidden;
        opacity: 0.8;
        -webkit-line-clamp: 2;
        -webkit-box-orient: vertical;
    }

    .kg-bookmark-metadata {
        margin-top: 20px;
    }

    .kg-bookmark-metadata {
        display: flex;
        align-items: center;
        font-weight: 500;
        font-size: 1.3rem;
        line-height: 1.3em;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
    }

    .kg-bookmark-description {
        display: -webkit-box;
        -webkit-box-orient: vertical;
        -webkit-line-clamp: 2;
        overflow: hidden;
    }

    .kg-bookmark-metadata amp-img {
        width: 18px;
        height: 18px;
        max-width: 18px;
        max-height: 18px;
        margin-right: 10px;
    }

    .kg-bookmark-thumbnail {
        display: flex;
        flex-basis: 20rem;
        flex-grow: 1;
        justify-content: flex-end;
    }

    .kg-bookmark-thumbnail amp-img {
        max-height: 200px;
    }

    .kg-bookmark-author {
        white-space: nowrap;
        text-overflow: ellipsis;
        overflow: hidden;
    }

    .kg-bookmark-publisher::before {
        content: "•";
        margin: 0 .5em;
    }

    .kg-width-full.kg-card-hascaption {
        display: grid;
        grid-template-columns: inherit;
    }

    .post-content table {
        border-collapse: collapse;
        width: 100%;
    }

    .post-content th {
        padding: 0.5em 0.8em;
        text-align: left;
        font-size: .75em;
        text-transform: uppercase;
    }

    .post-content td {
        padding: 0.4em 0.7em;
    }

    .post-content tbody tr:nth-child(2n + 1) {
        background-color: rgba(0,0,0,0.1);
        padding: 1px;
    }

    .post-content tbody tr:nth-child(2n + 2) td:last-child {
        box-shadow:
            inset 1px 0 rgba(0,0,0,0.1),
            inset -1px 0 rgba(0,0,0,0.1);
    }

    .post-content tbody tr:nth-child(2n + 2) td {
        box-shadow: inset 1px 0 rgba(0,0,0,0.1);
    }

    .post-content tbody tr:last-child {
        border-bottom: 1px solid rgba(0,0,0,.1);
    }

    .page-footer {
        padding: 60px 5vmin;
        margin: 60px auto 0;
        text-align: center;
        background-color: #f8f8f8;
    }

    .page-footer h3 {
        margin: 0.5rem 0 0 0;
    }

    .page-footer p {
        max-width: 500px;
        margin: 1rem auto 1.5rem;
        font-size: 1.7rem;
        line-height: 1.5em;
        color: rgba(0,0,0,0.6)
    }

    .powered {
        display: inline-flex;
        align-items: center;
        margin: 30px 0 0;
        padding: 6px 9px 6px 6px;
        border: rgba(0,0,0,0.1) 1px solid;
        font-size: 12px;
        line-height: 12px;
        letter-spacing: -0.2px;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Oxygen", "Ubuntu", "Cantarell", "Fira Sans", "Droid Sans", "Helvetica Neue", sans-serif;
        font-weight: 500;
        color: #222;
        text-decoration: none;
        background: #fff;
        border-radius: 6px;
    }

    .powered svg {
        height: 16px;
        width: 16px;
        margin: 0 6px 0 0;
    }

    @media (max-width: 600px) {
        body {
            font-size: 1.6rem;
        }
        h1 {
            font-size: 3rem;
        }

        h2 {
            font-size: 2.2rem;
        }
    }

    @media (max-width: 400px) {
        h1 {
            font-size: 2.6rem;
            line-height: 1.15em;
        }
        h2 {
            font-size: 2rem;
            line-height: 1.2em;
        }
        h3 {
            font-size: 1.7rem;
        }
    }

    :root {--ghost-accent-color: #650179;}
    </style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    

</head>

<body class="amp-template">
    <header class="page-header">
        <a href="../../index.html">
                sffresch
        </a>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">DIY neural network (part 1)</h1>
                <section class="post-meta">
                    Frank Eschner -
                    <time class="post-date" datetime="2021-08-17">17 Aug 2021</time>
                </section>
            </header>
            <figure class="post-image">
                <amp-img src="https://images.unsplash.com/photo-1505235687559-28b5f54645b7?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDF8fHNlZWRsaW5nfGVufDB8fHx8MTY0MTIzODkzNg&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000" width="600" height="340" layout="responsive"></amp-img>
            </figure>
            <section class="post-content">

                <p>This is the first post of a series about creating your very own artificial neural network (ANN) by hand. We are going to start as simple as it can get: Creating a baby neural network that can barely solve problems each of us could easily solve by hand. In each part of this series, we will continue to improve the capabilities of our self-made ANN and try to tackle more challenging problems with it.</p><p><strong>TL;DR</strong>: <a href="https://github.com/frank690/sffresch-code/blob/main/diy-neural-network-part-1/code.ipynb">Take me to the code</a>.</p><h3 id="the-problem">The Problem</h3><p>Consider having three light bulbs. The first two of them (A and B) can be turned either ON (1) or OFF (0) <em>independently</em>. The state of the third light bulb (C) depends on the first two (A and B). Try to figure out how the underlying systematic works by clicking on A and/or B as much as you like.</p><div class="lightbulbs">
    <div class="lightbulb">
        
    	<span class="caption">A</span>
	</div>
	<div class="lightbulb">
        
    	<span class="caption">B</span>
	</div>
	<div class="lightbulb">
        
    	<span class="caption">C</span>
	</div>
</div><p>The presented problem is known as the <a href="https://en.wikipedia.org/wiki/XNOR_gate">XNOR gate</a>. If A and B are both either 0 or 1, C will be 1. Otherwise, C will be 0. The corresponding input data (the states of A and B) will be stored in the variable X and the output data (the state of C) as y.</p><h3 id="generating-more-data">Generating more data</h3><p><s>Unfortunately,</s> Luckily, the data we got is a bit noisy on the input side and we do not get a clean 0 or 1 for the states of A and B, but rather a value around (!) 0 or 1. Since it is very unlikely that two samples are going to be the very same, we can now easily draw way more data than the initial four possible states (of A and B).</p><pre><code class="language-python">import numpy as np
from typing import Tuple, Dict

def generate_data(N: int) -&gt; Tuple[np.ndarray, np.ndarray]:
    """
    Original code credits to Prof. Dr. Stefan Harmeling
    
    Generate a data for training our linear model.
    :param N: number of samples multiplier.
    :return: tuple of x and y data as numpy ndarrays.
    """
    X = np.repeat(np.array([[0, 0], [0, 1], [1, 0], [1, 1]]), N, axis=0)
    X = X + np.random.randn(4 * N, 2) * 0.2
    y = np.repeat([0, 1, 1, 0], N)
    y = np.reshape(y, (len(y), 1))

    return X, y</code></pre><p>Using the generate_data function gets us some nice training and testing sets to feed to our ANN.</p><pre><code class="language-python">X_train, y_train = generate_data(N=100)
X_test, y_test = generate_data(N=50)</code></pre><figure class="kg-card kg-image-card"></figure><h3 id="initialize-weights-and-biases">Initialize weights and biases</h3><p>Since we now know what our data looks like, let us define a simple ANN with one layer each for the input, hidden, and output layers. The input layer will have two neurons (corresponding to the input data A and B) and the output layer will have one neuron (C). We choose the number of hidden neurons to be 3.</p><p>Side note: <br />There is no general answer to the question of choosing how many hidden layers and neurons per (hidden) layer one should use. In my personal experience, this highly depends on multiple factors. To name just a few, one is of course the complexity of the problem that the ANN should try to solve. Another one is the computing time one can spend and the capabilities of the machine that trains the ANN. Generally speaking, one should stick with <a href="https://en.wikipedia.org/wiki/Occam's_razor">Occam's razor</a>: If you have two similarly performing models, choose the simpler one.</p><pre><code class="language-python">weights = dict()
biases = dict()

weights[1] = 2 * np.random.random((2,3)) - 1
biases[1] = np.zeros(3)

weights[2] = 2 * np.random.random((3,1)) - 1
biases[2] = np.zeros(1)</code></pre><h3 id="forward-propagation">Forward propagation</h3><p>Given some input data, compute the resulting prediction (hypothesis). We introduce non-linearity to our ANN by using the well-known <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid function</a> (and its derivative).</p><pre><code class="language-python">def sigmoid(X: np.ndarray) -&gt; np.ndarray:
    """
    The sigmoid function will return 0 for every x &lt;&lt; 0 and 1 for every x &gt;&gt; 0.
    :param X: data to transform via sigmoid function:
    :return: transformed data that lies between 0 and 1.
    """
    return 1 / (1 + np.exp(-X))</code></pre><pre><code class="language-python">def sigmoid_derivative(X: np.ndarray) -&gt; np.ndarray:
    """
    The derivative of the sigmoid function.
    :param X: the data points for that the slope of the sigmoid function should be returned.
    :return: the slope of the sigmoid function at every given X.
    """
    sig = sigmoid(X)
    return sig * (1 - sig)</code></pre><p>Side note:<br />Instead of computing the resulting values for one sample at a time (via for-loops), I prefer to use the vectorized version of the same computations due to cleaner code and faster computations.</p><pre><code class="language-python">z = dict()
a = dict()

z[1] = X @ weights[1] + biases[1]
a[1] = sigmoid(z[1])

z[2] = a[1] @ weights[2] + biases[2]
a[2] = sigmoid(z[2])

h = a[2]</code></pre><h3 id="computing-the-loss">Computing the loss</h3><p>Since we are trying to solve a binary classification problem (predicting if the result is either 0 or 1), we will use the <a href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression">cross-entropy loss function</a>.</p><pre><code class="language-python">def cross_entropy_loss(
    h: np.ndarray, y: np.ndarray
) -&gt; np.ndarray:
    """
    Compute the cross entropy loss for the given hypothesis (h) in contrast to the true results (y).
    :param h: Hypothesis of the NN to compare with y.
    :param y: True results of the data.
    :return: Cost/Loss of the current hypothesis.
    """
    return -(1/y.size) * ((y.T @ np.log(h)) + ((1 - y.T) @ np.log(1 - h)))</code></pre><pre><code class="language-python">def cross_entropy_derivative(
    h: np.ndarray, y: np.ndarray
) -&gt; np.ndarray:
    """
    Compute the derivative of the cross entropy loss.
    :param h: Hypothesis of the NN to compare with y.
    :param y: True results of the data.
    :return: Derivative of the cross entropy loss with the current hypothesis.
    """
    return -(y // h) + ((1 - y) // (1 - h))</code></pre><h3 id="backpropagation">Backpropagation</h3><p>After computing the prediction of our neural network (also known as the hypothesis), we now work our way back(wards) to the weights and biases. This is done by looking at the loss and computing the gradients of each of our previously computed values.</p><pre><code class="language-python">dz = dict()
da = dict()

da[2] = cross_entropy_derivative(h=h, y=y)
dz[2] = da[2] * sigmoid_derivative(z[2])

da[1] = dz[2] @ weights[2].T
dz[1] = da[1] * sigmoid_derivative(z[1])</code></pre><h3 id="gradient-descent">Gradient descent</h3><p>To update our weights and biases so that the next prediction of our neural network will be a bit better than the previous one, we use <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>. For simplicity, let us choose a learning rate of 1 (and thus omit it in the code).</p><pre><code class="language-python">weights[1] -= (X.T @ dz[1]) * (1 / y.size)
biases[1] -= np.mean(dz[1], axis=0)

weights[2] -= (a[1].T @ dz[2]) * (1 / y.size)
biases[2] -= np.mean(dz[2], axis=0)</code></pre><h3 id="putting-it-all-together">Putting it all together</h3><p>Since we are descending one step at a time towards a minimum (due to the way how gradient descent works), we repeat the process of forward- and backpropagation through the network a couple of times (here, 1000 times). Putting this information plus every previous step into a single function called fit, our ANN is ready to go.</p><pre><code class="language-python">def fit(X: np.ndarray, y: np.ndarray) -&gt; Tuple[Dict, Dict]:
    """
    Create a simple ANN and train it on the given binary classification data.
    :param X: Input data.
    :param y: Output data.
    :return: Weights and biases as dictionaries.
    """
    weights = dict()
    biases = dict()

    weights[1] = 2 * np.random.random((2,3)) - 1
    biases[1] = np.zeros(3)
    weights[2] = 2 * np.random.random((3,1)) - 1
    biases[2] = np.zeros(1)

    a = dict()
    z = dict()
    da = dict()
    dz = dict()

    for iteration in range(1000):
        z[1] = X @ weights[1] + biases[1]
        a[1] = sigmoid(z[1])
        z[2] = a[1] @ weights[2] + biases[2]
        a[2] = sigmoid(z[2])
        h = a[2]

        loss = cross_entropy_loss(h=h, y=y)
        print(f"Loss ({iteration}): {loss[0][0]}")

        da[2] = cross_entropy_derivative(h=h, y=y)
        dz[2] = da[2] * sigmoid_derivative(z[2])
        da[1] = dz[2] @ weights[2].T
        dz[1] = da[1] * sigmoid_derivative(z[1])

        weights[1] -= (X.T @ dz[1]) * (1 / y.size)
        biases[1] -= np.mean(dz[1], axis=0)

        weights[2] -= (a[1].T @ dz[2]) * (1 / y.size)
        biases[2] -= np.mean(dz[2], axis=0)

    return weights, biases</code></pre><h3 id="training">Training</h3><p>Using our previously created training set, we can run the fit function and train our ANN. In return, we will get the fitted weights and biases that represent the <em>brain</em> of our ANN.</p><pre><code class="language-python">w, b = fit(X=X_train, y=y_train)</code></pre><pre><code class="language-shell">Loss (0): 0.701788446507631
Loss (1): 0.6932204170602805
Loss (2): 0.6926989143616228
Loss (3): 0.6925255938589576
...
Loss (997): 0.08240692560029927
Loss (998): 0.08234153724021534
Loss (999): 0.08227351128655165</code></pre><h3 id="predicting">Predicting</h3><p>To predict the output of data our ANN has not seen before, we need a dedicated predict function that takes the fitted weights and biases as input parameters.</p><pre><code class="language-python">def predict(w: Dict, b: Dict, X: np.ndarray) -&gt; np.ndarray:
    """
    Use the given weights (w) and biases (b) to make a prediction for the given input data (X).
    :param w: Dictionary of weight matrices.
    :param b: Dictionary of bias vectors.
    :param X: Input data to make predictions on.
    :return: An numpy array of predictions for every sample in X.
    """
    z = dict()
    a = dict()
    
    z[1] = X @ w[1] + b[1]
    a[1] = sigmoid(z[1])
    z[2] = a[1] @ w[2] + b[2]
    a[2] = sigmoid(z[2])
    
    return a[2]</code></pre><h3 id="results">Results</h3><p>Looking at the course of the loss, we can see that our ANN was able to fit the given data in such a way, that the resulting loss (for the training data) decreased significantly. Plotting the testing dataset together with the decision boundaries, one can see that our ANN successfully learned to classify data of the XNOR problem (nearly always) correctly. </p><figure class="kg-card kg-image-card"></figure><h3 id="what-is-next">What is next?</h3><p>Now having a baby ANN at our fingertips, there are plenty of things to implement and improve to solve more challenging problems. This includes introducing <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">stochastic gradient descent</a>, different kinds of regularization methods, <a href="https://golden.com/wiki/Nesterov_momentum">Nesterov momentum</a>, <a href="https://arxiv.org/abs/1502.01852">more sophisticated weight + bias initialization</a>, <a href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html">more loss functions</a>, <a href="https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html">more activation functions</a>, etc. But besides the mathematical perspective, there is also a lot of room for improving the code quality.</p><h3 id="acronyms">Acronyms</h3><p>ANN - Artificial Neural Network<br /></p>

            </section>

        </article>
    </main>
    <footer class="page-footer">
        <h3>sffresch</h3>
            <p>a blog about my projects</p>
        <p><a href="../../index.html">Read more posts →</a></p>
        <a class="powered" href="https://ghost.org" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 156 156"><g fill="none" fill-rule="evenodd"><rect fill="#15212B" width="156" height="156" rx="27"/><g transform="translate(36 36)" fill="#F6F8FA"><path d="M0 71.007A4.004 4.004 0 014 67h26a4 4 0 014 4.007v8.986A4.004 4.004 0 0130 84H4a4 4 0 01-4-4.007v-8.986zM50 71.007A4.004 4.004 0 0154 67h26a4 4 0 014 4.007v8.986A4.004 4.004 0 0180 84H54a4 4 0 01-4-4.007v-8.986z"/><rect y="34" width="84" height="17" rx="4"/><path d="M0 4.007A4.007 4.007 0 014.007 0h41.986A4.003 4.003 0 0150 4.007v8.986A4.007 4.007 0 0145.993 17H4.007A4.003 4.003 0 010 12.993V4.007z"/><rect x="67" width="17" height="17" rx="4"/></g></g></svg> Published with Ghost</a>
    </footer>
    
</body>
</html>

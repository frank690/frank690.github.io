<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>DIY neural network (part 2)</title>
    <link rel="stylesheet" href="../assets/built/screen.css%3Fv=f5d2ac8d11.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700,700i%7CDosis:600,700&amp;subset=latin-ext">
        
    <link rel="canonical" href="index.html" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    <link rel="amphtml" href="amp/index.html" />
    
    <meta property="og:site_name" content="sffresch" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="DIY neural network (part 2)" />
    <meta property="og:description" content="This post is all about restructuring the code of our previously made neural network so we can use it in a way more elegant way, much like frameworks like Tensorflow or PyTorch." />
    <meta property="og:url" content="http://frank690.github.io/diy-neural-network-part-2/" />
    <meta property="og:image" content="https://images.unsplash.com/photo-1444530495635-029990f82ce8?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDE0fHxzZWVkbGluZ3xlbnwwfHx8fDE2NTIzNTM0NTY&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000" />
    <meta property="article:published_time" content="2022-05-12T11:02:59.000Z" />
    <meta property="article:modified_time" content="2022-05-15T21:17:29.000Z" />
    <meta property="article:tag" content="ANN" />
    <meta property="article:tag" content="DIY" />
    <meta property="article:tag" content="neural-network" />
    <meta property="article:tag" content="python" />
    
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="DIY neural network (part 2)" />
    <meta name="twitter:description" content="This post is all about restructuring the code of our previously made neural network so we can use it in a way more elegant way, much like frameworks like Tensorflow or PyTorch." />
    <meta name="twitter:url" content="http://frank690.github.io/diy-neural-network-part-2/" />
    <meta name="twitter:image" content="https://images.unsplash.com/photo-1444530495635-029990f82ce8?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDE0fHxzZWVkbGluZ3xlbnwwfHx8fDE2NTIzNTM0NTY&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Frank Eschner" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="ANN, DIY, neural-network, python" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="1325" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "sffresch",
        "url": "http://frank690.github.io/",
        "logo": {
            "@type": "ImageObject",
            "url": "http://frank690.github.io/favicon.ico",
            "width": 48,
            "height": 48
        }
    },
    "author": {
        "@type": "Person",
        "name": "Frank Eschner",
        "image": {
            "@type": "ImageObject",
            "url": "http://frank690.github.io/content/images/2021/08/68586209_10205828296182785_2060227135364136960_n.jpg",
            "width": 1536,
            "height": 2048
        },
        "url": "http://frank690.github.io/author/frank690/",
        "sameAs": [
            "https://sffresch.de"
        ]
    },
    "headline": "DIY neural network (part 2)",
    "url": "http://frank690.github.io/diy-neural-network-part-2/",
    "datePublished": "2022-05-12T11:02:59.000Z",
    "dateModified": "2022-05-15T21:17:29.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://images.unsplash.com/photo-1444530495635-029990f82ce8?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDE0fHxzZWVkbGluZ3xlbnwwfHx8fDE2NTIzNTM0NTY&ixlib=rb-1.2.1&q=80&w=2000",
        "width": 2000,
        "height": 1325
    },
    "keywords": "ANN, DIY, neural-network, python",
    "description": "This post is all about restructuring the code of our previously made neural network so we can use it in a way more elegant way, much like frameworks like Tensorflow or PyTorch. ",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://frank690.github.io/"
    }
}
    </script>

    <meta name="generator" content="Ghost 4.17" />
    <link rel="alternate" type="application/rss+xml" title="sffresch" href="../rss/index.html" />
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-KGHY4JLHRR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-KGHY4JLHRR');
</script>

<!-- Inline Latex Math -->
<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']],
     processEscapes: true}          
   });
</script>

<!-- Latex Math -->
<script type="text/javascript" async
src="https://cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!--- copy to clipboard -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism-tomorrow.min.css" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/plugins/toolbar/prism-toolbar.min.css" />

<!--- code styling -->
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.6.0/themes/prism-okaidia.min.css" rel="stylesheet" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.6.0/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet" />

<!--- cookie consent -->
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.css" /><style>:root {--ghost-accent-color: #650179;}</style>
</head>

<body class="post-template tag-ann tag-diy tag-neural-network tag-python">
<div id='particles-js'></div>
    <div class="site">
        <header class="site-header">
    <div class="navbar">
        <div class="navbar-left">
            <a class="logo" href="../index.html">
        <span class="logo-text">sffresch</span>
</a>        </div>
            <nav class="main-menu hidden-xs hidden-sm hidden-md">
                <ul class="nav-list u-plain-list">
        <li
            class="menu-item menu-item-whoami">
            <a class="menu-item-link" href="../whoami/index.html">whoami</a>
        </li>
        <li
            class="menu-item menu-item-projects">
            <a class="menu-item-link" href="../projects.html">projects</a>
        </li>
</ul>
            </nav>
        <div class="navbar-right">
            <div class="social hidden-xs hidden-sm"></div>
            <div class="burger hidden-lg hidden-xl">
    <div class="burger-bar"></div>
    <div class="burger-bar"></div>
</div>        </div>
    </div>
</header>        <div class="site-content">
            
<div class="container">
    <div class="row">
        <div class="content-column col-sm-12">
            <div class="content-area">
                <main class="site-main">
                        <article
                            class="post tag-ann tag-diy tag-neural-network tag-python single u-shadow">
                            <div class="post-meta">
                                <time class="post-meta-date"
                                    datetime="2022-05-12">
                                    May 12, 2022
                                </time>
                                <span
                                    class="post-meta-length">7 min read</span>
                            </div>
                                <figure class="post-media">
        <div class="u-placeholder">
            <a class="post-image-link" href="index.html">
            <div class="on-media-title no-select">
                DIY neural network (part 2)
            </div>
            <img class="post-image no-select lazyload"
                data-srcset="https://images.unsplash.com/photo-1444530495635-029990f82ce8?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDE0fHxzZWVkbGluZ3xlbnwwfHx8fDE2NTIzNTM0NTY&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000 400w, https://images.unsplash.com/photo-1444530495635-029990f82ce8?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDE0fHxzZWVkbGluZ3xlbnwwfHx8fDE2NTIzNTM0NTY&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000 750w, https://images.unsplash.com/photo-1444530495635-029990f82ce8?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDE0fHxzZWVkbGluZ3xlbnwwfHx8fDE2NTIzNTM0NTY&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000 960w"
                src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="
                data-sizes="auto" alt="DIY neural network (part 2)">
            </a>
        </div>

                <figcaption>Photo by <a href="https://unsplash.com/@adelau1?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit">Austin D</a> / <a href="https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit">Unsplash</a></figcaption>
    </figure>
                            <div class="post-wrapper">
                                <div class="post-content u-text-format">
                                    <p>This post is all about restructuring the <a href="https://github.com/frank690/sffresch-code/blob/main/diy-neural-network-part-1/code.ipynb">code</a> of our <a href="../diy-neural-network-part-1.html">previously made neural network</a> so we can use it in a way more elegant way, much like frameworks like <a href="https://www.tensorflow.org/">Tensorflow</a> or <a href="https://pytorch.org/">PyTorch</a>. For this purpose, we will construct a base class and consistently add more features and complexity to it within this and the following parts of the series.</p><p><strong>TL;DR</strong>: <a href="https://github.com/frank690/sffresch-code/blob/main/diy-neural-network-part-2/code.ipynb">Take me to the nice code</a>.</p><h3 id="the-base-class">The base class</h3><p>Artificial neural networks (ANNs) can consist of a variety of differently shaped layers and have various activation and loss functions. Being able to choose from these possibilities, when creating an ANN (programmatically) easily and elegantly, is key. First, we will come up with a class representation for a simple ANN that expects a list of integers as input, which represent the number of neurons in each layer.</p><pre><code class="language-python">class ANN:
    """
    Class representation of an artificial neural network (ANN).
    """
    def __init__(self, layers: List[int]):
        """
        Initialization function to set up the class.
        :param layers: Number of neurons for each layer that should be set up as List of ints.
        """
        self.W = dict()  # holding the weight matrices
        self.b = dict()  # holding the bias values
        self.z = dict()  # holding the intermediate values
        self.a = dict()  # holding the activation values
        
        self.dW = dict()  # holding the gradient of the weight matrices
        self.db = dict()  # holding the gradient of the bias values
        
        self.layers = layers  # User defined layers

        self._construct()
        
    def _construct(self):
        """
        Construct the internal shape of the ANN.
        """
        for idx, layer in enumerate(self.layers[:-1]):
            self.W[idx] = np.random.randn(layer, self.layers[idx+1])
            self.b[idx] = np.zeros(self.layers[idx+1])</code></pre><p>With the information about the neurons in each layer, the basic weight and bias structure (which will be optimized during the training process) is implemented. For now, the initial weight values are derived from a <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal Gaussian distribution</a> and the bias values are set to plain zeros. Note that during the class initialization, there are four other dictionaries added to the class parameters. These hold information about the derivates of the weights ($dW$) and biases ($db$) but also about some intermediate ($z$) and activation values ($a$).</p><h3 id="the-cost-function">The cost function</h3><p>Depending on what kind of problem you want to solve, a variety of cost functions are applicable. Here we are going to stick with the cross-entropy-loss, that we already introduced in the <a href="../diy-neural-network-part-1.html">previous post of this series</a>. We accomplish this by adding the following method to our base class. Note that by providing a boolean true as the gradient parameter, we return the derivative of the loss. At a later point, we will provide a variety of loss functions and (ideally) grant the user the opportunity to implement custom functions, as well.</p><pre><code class="language-python">def loss(self, y: np.ndarray, gradient: bool = False) -&gt; np.ndarray:
    """
    Compute the cross entropy loss for the given hypothesis (h) in contrast to the true results (y).
    If the gradient flag is True, the derivative of said loss function will be returned.
    :param y: True output data.
    :param gradient: Bool flag to indicate if gradient should be returned.
    :return: Cost/Loss of the current hypothesis.
    """
    h = self.a[len(self.layers)-2]  # our prediction / hypothesis

    if gradient:
    	return -(y // h) + ((1 - y) // (1 - h))
    return -(1/y.size) * ((y.T @ np.log(h)) + ((1 - y.T) @ np.log(1 - h)))</code></pre><h3 id="the-activation-function">The activation function</h3><p>Just like the cost function, the activation function also highly depends on the problem that one wants to solve. One of the classic approaches is to use the <a href="https://en.wikipedia.org/wiki/Sigmoid_function">Sigmoid function</a>. We add this functionality with the following code snippet. Also here, we are granting the user the opportunity to get the gradient of the activation function by passing the boolean gradient flag, accordingly.</p><pre><code class="language-python">def activation(self, X: np.ndarray, gradient: bool = False) -&gt; np.ndarray:
    """
    For the activation function we use the sigmoid.
    It will return 0 for every x &lt;&lt; 0 and 1 for every x &gt;&gt; 0.
    Return the gradient of the sigmoid if a True gradient flag is given.
    :param X: data to transform via sigmoid function:
    :return: transformed data that lies between 0 and 1.
    """
    sigmoid = 1 / (1 + np.exp(-X))
    if gradient:
    	return sigmoid * (1 - sigmoid)
    return sigmoid</code></pre><h3 id="propagate-forward">Propagate forward</h3><p>Part of training an ANN and also a mandatory part to make predictions is the ability to propagate forward. Since the number of layers is decided upon the class initialization, we have to loop over all given weights and biases and compute the resulting intermediate ($z$) and activation values ($a$). The computed values will then be stored in the already existing dictionaries for said values.</p><pre><code class="language-python">def forward(self, X: np.ndarray):
    """
    Successively propagate the input data (X) through the ANN and store all
    intermediate and activation values in their corresponding dictionaries.
    :param X: Input data to make predictions on.
    """
    self.a[-1] = X
    for idx in range(len(self.layers)-1):
        self.z[idx] = self.a[idx-1] @ self.W[idx] + self.b[idx]
        self.a[idx] = self.activation(self.z[idx])</code></pre><h3 id="propagate-backward">Propagate backward</h3><p>Once we propagated forward through the whole ANN, our hypothesis (e.g. prediction) is represented by the output of the latest activation value. Next, we compute the loss and work our way backward through the network until we reached (computed) all derivations (gradients) of all our weights and biases. With these derivations ($dW$ and $db$) we head on to update our weights and biases.</p><pre><code class="language-python">def backward(self, y: np.ndarray):
    """
    Successively propagate the prediction as well as the true output backwards through the ANN.
    Store the resulting gradients for weights and biases in their corresponding dictionaries.
    :param y: True output data.
    """
    da = self.loss(y=y, gradient=True)  # get gradient of last activation value

    for idx in range(len(self.layers)-2, -1, -1):  # loop from the last layer to zero (effectively)
        dz = da * self.activation(X=self.z[idx], gradient=True)
        da = dz @ self.W[idx].T  

        self.db[idx] = np.mean(dz, axis=0)  # get gradient of bias. use mean to pay respect to sample size.
        self.dW[idx] = (self.a[idx-1].T @ dz) / y.size  # get gradient of weights. divide by number of samples.  </code></pre><h3 id="predict">Predict</h3><p>To predict the output, given some data, all we need to do is propagate said data forward through the network and look at the resulting hypothesis (e.g. activation value of the very last layer). Since we are only dealing with binary classification at this point, we apply an (arbitrary) threshold to the resulting predictions. This results in the predictions of our network being absolute (either class 0 or 1) for each provided sample.</p><pre><code class="language-python">def predict(self, X: np.ndarray, threshold: float = 0.5) -&gt; np.ndarray:
    """
    Predict the output of the given data (X).
    :param X: Data to make prediction on.
    :param threshold: Threshold that decides if predicted value belongs to class 0 or 1.
    :return: Predicted value.
    """
    self.forward(X)
    return self.a[len(self.layers)-2] &gt; threshold
</code></pre><h3 id="update">Update</h3><p>After computing the gradients of our weights ($dW$) and biases ($db$) during backpropagation, we might as well use them now to update said weights and biases. Since we stored the gradients in their own dictionaries (which is a parameter of the class), it is an easy job to call them and to do the updating. In contrast to our previous code, we introduced the learning rate. This is a mere factor that scales the strength of the applied update. Previously, we just set the learning rate to be a fixed 1 (and thus omitted it in the code).</p><pre><code class="language-python">def update(self, learning_rate: float):
    """
    Update the current weights and biases by multiplying the learning rate with the previously computed gradients.
    :param learning_rate: The step size of the gradient applied to update the weights and biases (e.g. to learn).
    """
    for idx in range(len(self.layers)-1):
        self.W[idx] -= learning_rate * self.dW[idx]
        self.b[idx] -= learning_rate * self.db[idx]  </code></pre><h3 id="training">Training</h3><p>Now that we have integrated the methods for working our way through the network (forth and back), computing the loss of each sample as well as updating the weights and biases, we can put everything together to implement the training procedure.</p><pre><code class="language-python">def fit(self, X: np.ndarray, y: np.ndarray, learning_rate: float = 1, epochs: int = 1000):
    """
    Run the training procedure on the given data for the given epochs.
    This essentially fits the network to the given data.
    Print current loss value every epoch.
    :param X: Input data.
    :param y: True output data.
    :param learning_rate: The step size of the gradient applied to update the weights and biases (e.g. to learn).
    :param epochs: Number of training cycles to perform.
    """
    for epoch in range(epochs):
        self.forward(X=X)
        loss = self.loss(y=y)
        self.backward(y=y)
        self.update(learning_rate)

        print(f"({epoch+1}/{epochs}): {loss.item()}")</code></pre><h3 id="accuracy">Accuracy</h3><p>Instead of <em>just</em> refactoring our previous code into a nice and handy class, we might as well add something new to it. Having a metric to know how accurate your (trained) model performs on a given dataset, is quite valuable. So this is what we are going to implement. Accuracy describes the ratio of correctly identified classes to all the classifications it predicted (correct and incorrect ones).</p><pre><code class="language-python">def accuracy(self, X: np.ndarray, y: np.ndarray) -&gt; float:
    """
    Perform predictions on all the given data (X) and compare these predictions to the ground truth values (y).
    Afterwards get the ratio of correctly to correctly+incorrectly predicted classes. This is the accuracy.
    :param X: Input data.
    :param y: True output data.
    :return: Accuracy as float value.
    """
    return np.sum(self.predict(X) == y) / y.size</code></pre><h3 id="testing">Testing</h3><p>Using the same XNOR data we had in the <a href="../diy-neural-network-part-1.html">previous post of this series</a>, we can confirm that we see near-identical results. Also, our accuracy on both, training and testing data confirms that we fitted our model (ANN) successfully to the generated data.</p><pre><code class="language-python">model = ANN([2,3,1])         # initialize our ANN model
model.fit(X_train, y_train)  # fit it to our training data</code></pre><pre><code class="language-shell">(1/1000): 0.8624013698343836
(2/1000): 0.7502329385727288
(3/1000): 0.7085321817003911
...
(998/1000): 0.09573808006389115
(999/1000): 0.0956405468534334
(1000/1000): 0.0955433438225078</code></pre><pre><code class="language-python">model.accuracy(X_train, y_train)  # 0.975
model.accuracy(X_test, y_test)    # 0.955</code></pre><h3 id="summary">Summary</h3><p>And that's it! We successfully refactored our previous code into a single class that we can now use to easily create ANNs with arbitrary sizes (layers and number of neurons). Admittingly, there is still plenty of work to do from this point on. In the next posts of this series, we will add new capabilities to our ANN as well as continually improve its code structure, so that it always stays convenient for us to use. Click <a href="https://github.com/frank690/sffresch-code/blob/main/diy-neural-network-part-2/code.ipynb">here</a> to see the complete code of this post in a jupyter notebook.</p><p>Thank you for reading!</p>
                                </div>
                                    <h2 class="post-tags-header">Tags</h2>
    <div class="post-tags">
            <a class="post-tag" id="ANN" href="../tag/ann/index.html" style="background-color: #8e0101;">ANN</a>
            <a class="post-tag" id="DIY" href="../tag/diy/index.html" style="background-color: #a87d34;">DIY</a>
            <a class="post-tag" id="neural-network" href="../tag/neural-network/index.html" style="background-color: #600094;">neural-network</a>
            <a class="post-tag" id="python" href="../tag/python/index.html" style="background-color: #026600;">python</a>
    </div>
                            </div>
                        </article>
                </main>
            </div>
        </div>
    </div>
</div>
        </div>
        <footer class="site-footer">
    <div class="container">
        <div class="footer-inner">
            <div class="social">
                <a class="social-item social-item-rss"
                    href="https://feedly.com/i/subscription/feed/http://frank690.github.io/rss/"
                    target="_blank" rel="noopener noreferrer" aria-label="RSS">
                    <i class="icon icon-rss"></i>
                </a>
            </div>
            <div class="copyright">
                Powered by <a href="https://ghost.org/" target="_blank">Ghost</a>
            </div>
        </div>
    </div>
</footer>    </div>


    <div class="dimmer"></div>
<div class="off-canvas">
    <div class="burger burger-close hidden-lg hidden-xl">
    <div class="burger-bar"></div>
    <div class="burger-bar"></div>
</div>    <div class="mobile-menu">
        <ul class="nav-list u-plain-list">
        <li
            class="menu-item menu-item-whoami">
            <a class="menu-item-link" href="../whoami/index.html">whoami</a>
        </li>
        <li
            class="menu-item menu-item-projects">
            <a class="menu-item-link" href="../projects.html">projects</a>
        </li>
</ul>
    </div>
    <aside class="widget-area">
    <div class="widget widget-facebook widget-no-title u-shadow">
    <div class="fb-page" data-href="__YOUR_FACEBOOK_PAGE_URL__"
        data-small-header="false" data-hide-cover="false"
        data-show-facepile="true" data-hide-cta="false" data-tabs="none">
    </div>
</div>    <div class="widget widget-tags">
    <h4 class="widget-title">Tags</h4>
        <div class="tag-feed">
                <a class="post-tag" id="ANN" href="../tag/ann/index.html" style="background-color: #8e0101;">ANN</a>
                <a class="post-tag" id="DIY" href="../tag/diy/index.html" style="background-color: #a87d34;">DIY</a>
                <a class="post-tag" id="classification" href="../tag/classification/index.html" style="background-color: #517510;">classification</a>
                <a class="post-tag" id="data-science" href="../tag/data-science/index.html" style="background-color: #366356;">data-science</a>
                <a class="post-tag" id="log-loss" href="../tag/log-loss/index.html" style="background-color: #ff2424;">log-loss</a>
                <a class="post-tag" id="machine-learning" href="../tag/machine-learning/index.html" style="background-color: #001999;">machine-learning</a>
                <a class="post-tag" id="neural-network" href="../tag/neural-network/index.html" style="background-color: #600094;">neural-network</a>
                <a class="post-tag" id="python" href="../tag/python/index.html" style="background-color: #026600;">python</a>
                <a class="post-tag" id="softmax" href="../tag/softmax/index.html" style="background-color: #1c226d;">softmax</a>
                <a class="post-tag" id="theory" href="../tag/theory/index.html" style="background-color: #bf7e0d;">theory</a>
        </div>
</div></aside></div>
    <script src="https://code.jquery.com/jquery-3.3.1.min.js"
        integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
        crossorigin="anonymous">
        </script>
    <script src="../assets/built/main.min.js%3Fv=f5d2ac8d11"></script>

    <div id="fb-root"></div>
<script async defer crossorigin="anonymous"
    src="https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v3.2"></script>
    

    <!-- copy to clipboard -->
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/prism.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/plugins/toolbar/prism-toolbar.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.6.0/components/prism-python.min.js" ></script>
<script src="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.js" data-cfasync="false"></script>
<script>
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#ffffff"
    },
    "button": {
      "background": "#8ec760",
      "text": "#ffffff"
    }
  },
  "theme": "edgeless"
});
</script>
</body>

</html>
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Differentiate all the things</title>
    <link rel="stylesheet" href="../assets/built/screen.css%3Fv=a3ac031e72.css">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700,700i%7CDosis:600,700&amp;subset=latin-ext">
        
    <link rel="canonical" href="index.html" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    <link rel="amphtml" href="amp/index.html" />
    
    <meta property="og:site_name" content="sffresch" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Differentiate all the things" />
    <meta property="og:description" content="In this post, I am going to show you how to compute and implement the combination of the derivative of the softmax and the log-loss functions to run your gradient descent algorithm." />
    <meta property="og:url" content="http://frank690.github.io/differentiate-all-the-things/" />
    <meta property="og:image" content="https://images.unsplash.com/photo-1623387641168-d9803ddd3f35?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDJ8fGNhdHMlMjBhbmQlMjBkb2dzfGVufDB8fHx8MTY1MTU2MjAyMA&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000" />
    <meta property="article:published_time" content="2022-05-03T07:10:46.000Z" />
    <meta property="article:modified_time" content="2022-05-17T15:50:15.000Z" />
    <meta property="article:tag" content="ANN" />
    <meta property="article:tag" content="theory" />
    <meta property="article:tag" content="data-science" />
    <meta property="article:tag" content="softmax" />
    <meta property="article:tag" content="log-loss" />
    <meta property="article:tag" content="classification" />
    
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Differentiate all the things" />
    <meta name="twitter:description" content="In this post, I am going to show you how to compute and implement the combination of the derivative of the softmax and the log-loss functions to run your gradient descent algorithm." />
    <meta name="twitter:url" content="http://frank690.github.io/differentiate-all-the-things/" />
    <meta name="twitter:image" content="https://images.unsplash.com/photo-1623387641168-d9803ddd3f35?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDJ8fGNhdHMlMjBhbmQlMjBkb2dzfGVufDB8fHx8MTY1MTU2MjAyMA&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Frank Eschner" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="ANN, theory, data-science, softmax, log-loss, classification" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="1333" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "sffresch",
        "url": "http://frank690.github.io/",
        "logo": {
            "@type": "ImageObject",
            "url": "http://frank690.github.io/favicon.ico",
            "width": 48,
            "height": 48
        }
    },
    "author": {
        "@type": "Person",
        "name": "Frank Eschner",
        "image": {
            "@type": "ImageObject",
            "url": "http://frank690.github.io/content/images/2021/08/68586209_10205828296182785_2060227135364136960_n.jpg",
            "width": 1536,
            "height": 2048
        },
        "url": "http://frank690.github.io/author/frank690/",
        "sameAs": [
            "https://sffresch.de"
        ]
    },
    "headline": "Differentiate all the things",
    "url": "http://frank690.github.io/differentiate-all-the-things/",
    "datePublished": "2022-05-03T07:10:46.000Z",
    "dateModified": "2022-05-17T15:50:15.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://images.unsplash.com/photo-1623387641168-d9803ddd3f35?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=MnwxMTc3M3wwfDF8c2VhcmNofDJ8fGNhdHMlMjBhbmQlMjBkb2dzfGVufDB8fHx8MTY1MTU2MjAyMA&ixlib=rb-1.2.1&q=80&w=2000",
        "width": 2000,
        "height": 1333
    },
    "keywords": "ANN, theory, data-science, softmax, log-loss, classification",
    "description": "In this post, I am going to show you how to compute and implement the combination of the derivative of the softmax and the log-loss functions to run your gradient descent algorithm.",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://frank690.github.io/"
    }
}
    </script>

    <meta name="generator" content="Ghost 4.17" />
    <link rel="alternate" type="application/rss+xml" title="sffresch" href="../rss/index.html" />
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-KGHY4JLHRR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-KGHY4JLHRR');
</script>

<!-- Inline Latex Math -->
<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']],
     processEscapes: true}          
   });
</script>

<!-- Latex Math -->
<script type="text/javascript" async
src="https://cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!--- copy to clipboard -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism-tomorrow.min.css" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/plugins/toolbar/prism-toolbar.min.css" />

<!--- code styling -->
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.6.0/themes/prism-okaidia.min.css" rel="stylesheet" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.6.0/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet" />

<!--- cookie consent -->
<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.css" /><style>:root {--ghost-accent-color: #650179;}</style>
</head>

<body class="post-template tag-ann tag-theory tag-data-science tag-softmax tag-log-loss tag-classification">
<div id='particles-js'></div>
    <div class="site">
        <header class="site-header">
    <div class="navbar">
        <div class="navbar-left">
            <a class="logo" href="../index.html">
        <span class="logo-text">sffresch</span>
</a>        </div>
            <nav class="main-menu hidden-xs hidden-sm hidden-md">
                <ul class="nav-list u-plain-list">
        <li
            class="menu-item menu-item-whoami">
            <a class="menu-item-link" href="../whoami/index.html">whoami</a>
        </li>
        <li
            class="menu-item menu-item-projects">
            <a class="menu-item-link" href="../projects.html">projects</a>
        </li>
</ul>
            </nav>
        <div class="navbar-right">
            <div class="social hidden-xs hidden-sm"></div>
            <div class="burger hidden-lg hidden-xl">
    <div class="burger-bar"></div>
    <div class="burger-bar"></div>
</div>        </div>
    </div>
</header>        <div class="site-content">
            
<div class="container">
    <div class="row">
        <div class="content-column col-sm-12">
            <div class="content-area">
                <main class="site-main">
                        <article
                            class="post tag-ann tag-theory tag-data-science tag-softmax tag-log-loss tag-classification single u-shadow">
                            <div class="post-meta">
                                <time class="post-meta-date"
                                    datetime="2022-05-03">
                                    May 3, 2022
                                </time>
                                <span
                                    class="post-meta-length">10 min read</span>
                            </div>
                                <figure class="post-media">
        <div class="u-placeholder">
            <a class="post-image-link" href="index.html">
            <div class="on-media-title no-select">
                Differentiate all the things
            </div>
            <img class="post-image no-select lazyload"
                data-srcset="https://images.unsplash.com/photo-1623387641168-d9803ddd3f35?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDJ8fGNhdHMlMjBhbmQlMjBkb2dzfGVufDB8fHx8MTY1MTU2MjAyMA&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000 400w, https://images.unsplash.com/photo-1623387641168-d9803ddd3f35?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDJ8fGNhdHMlMjBhbmQlMjBkb2dzfGVufDB8fHx8MTY1MTU2MjAyMA&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000 750w, https://images.unsplash.com/photo-1623387641168-d9803ddd3f35?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDJ8fGNhdHMlMjBhbmQlMjBkb2dzfGVufDB8fHx8MTY1MTU2MjAyMA&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000 960w"
                src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="
                data-sizes="auto" alt="Differentiate all the things">
            </a>
        </div>

                <figcaption>Photo by <a href="https://unsplash.com/@sita2?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit">Andrew S</a> / <a href="https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit">Unsplash</a></figcaption>
    </figure>
                            <div class="post-wrapper">
                                <div class="post-content u-text-format">
                                    <p>In this post, I will show you how to compute and implement the combination of the derivative of the <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> and the <a href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression">log-loss</a> functions to run your <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a> algorithm.</p><p><strong>TL;DR</strong>: <a href="https://github.com/frank690/sffresch-code/blob/main/differentiate-all-the-things/code.ipynb">This is the code</a> and these are the derivatives. $ \frac{\delta L}{\delta w_{y}} =  - X_{n}^{T} * (1 - S_{y}), \frac{\delta L}{\delta w_{j}} = X_{n}^{T} * S_{j} $</p><h3 id="preface">Preface</h3><p>Let us assume that you want to create your neural network from scratch and you have decided upon using it for <a href="https://en.wikipedia.org/wiki/Multiclass_classification">multiclass classification</a> (MCC). There is a good chance, that you will choose the softmax in combination with the log-loss to do the job. Now to update the weights of your network accordingly, you need to know the solution to the differentiation of the whole thing (with respect to (wrt) the weights that you want to optimize).</p><p>$$ \frac{\delta L}{\delta w}=-\frac{1}{N}\sum_{n=1}^{N}log(\frac{e^{X_{n}w_{y}}}{\sum_{c=1}^{C}e^{X_{n}w_{c}}})\frac{\delta}{\delta w} $$</p><p>I highly encourage you to grab some pen and paper and try to come up with the derivatives yourself. In case you are stuck, just keep on reading until you know how to continue. To avoid confusion from the get-go, let us define what each term of the differentiation means and also what dimensions it has.</p><!--kg-card-begin: html--><table>
    <tr>
        <td><b>Symbol</b></td>
        <td style="white-space: nowrap"><b>Meaning</b></td>
        <td><b>Description</b></td>
    </tr>
    <tr>
    	<td>$ \delta $</td>
        <td style="white-space: nowrap">Delta</td>
        <td>The differentiation sign.</td>
    </tr>
    <tr>
    	<td>$ L $</td>
        <td style="white-space: nowrap">Overall loss</td>
        <td>This is what the log-loss ultimately computes.</td>
    </tr>
    <tr>
    	<td>$ X $</td>
        <td style="white-space: nowrap">Data</td>
        <td>Our input data of the MCC is a matrix. Imagine each row of the matrix is an actual image we want to classify.</td>
    </tr>
    <tr>
    	<td>$ w $</td>
        <td style="white-space: nowrap">Weight</td>
        <td>The weight of your neural network that you want to optimize.</td>
    </tr>
    <tr>
    	<td>$ -\frac{1}{N} $</td>
        <td style="white-space: nowrap">Minus 1 over N</td>
        <td>Divides by minus the number of samples (data points) we have in total.</td>
    </tr>
    <tr>
    	<td>$ \sum_{n=1}^{N} $</td>
        <td style="white-space: nowrap">Sum from n = 1 to N</td>
        <td>Sum the following expression over each sample, starting with 1 and ending with N.</td>
    </tr>
    <tr>
    	<td>$ log(...) $</td>
        <td style="white-space: nowrap">Logarithm</td>
        <td>Apply the logarithm to whatever is computed inside the brackets.</td>
    </tr>
    <tr>
    	<td>$ X_{n}w_{y} $</td>
        <td style="white-space: nowrap">Data times weights</td>
        <td>Compute the product of the n'th sample (row) of the input data with the y'th column of the weights. Note that y represents the (one) correct class of the given sample.</td>
    </tr>
    <tr>
    	<td>$ e^{...} $</td>
        <td style="white-space: nowrap">Exponential</td>
        <td>Compute e to the power of whatever is written in its exponential.</td>
    </tr>
    <tr>
    	<td>$ \sum_{c=1}^{C} $</td>
        <td style="white-space: nowrap">Sum from c = 1 to C</td>
        <td>Sum the following expression over each class, starting with 1 and ending with C.</td>
    </tr>
    <tr>
    	<td>$ X_{n}w_{c} $</td>
        <td style="white-space: nowrap">Data times weights</td>
        <td>Compute the product of the n'th sample (row) of the input data with the c'th column of the weights.</td>
    </tr>
    <tr>
    	<td>$ \frac{\delta}{\delta w} $</td>
        <td style="white-space: nowrap">Derivative wrt weights</td>
        <td>Since the whole previous term represents $ L $ but we still want to show that we want to differentiate this wrt the weights, we attach this term.</td>
    </tr>
    <tr><td><br /></td></tr>
    <tr>
        <td><b>Symbol</b></td>
        <td style="white-space: nowrap"><b>Dimension</b></td>
        <td><b>Description</b></td>
    </tr>
    <tr>
        <td>$ X $</td>
        <td style="white-space: nowrap">$ (N \times D) $</td>
        <td>Each row holds a sample of dimension D. There are N samples in total.</td>
    </tr>
    <tr>
        <td>$ w $</td>
        <td style="white-space: nowrap">$ (D \times C) $</td>
        <td>Each column represents a class of the classification problem (e.g. cat, dog, car, ...) in D dimensions. There are C classes in total.</b></td>
    </tr>
    <tr>
        <td>$ X_{n} $</td>
        <td style="white-space: nowrap">$ (1 \times D) $</td>
        <td>A single sample (row) with dimension D.</td>
    </tr>
    <tr>
        <td>$ w_{c} $</td>
        <td style="white-space: nowrap">$ (D \times 1) $</td>
        <td>The weights of a single class (column) with dimension D.</td>
    </tr>
</table><!--kg-card-end: html--><h3 id="divide">Divide ...</h3><p>By applying a bit of calculus, we can make our lives a lot easier. We are going to separate the softmax ($ S_{y} $) and the log-loss function ($ L $) from one another and differentiate them each at a time. Note the little index ($ y $) on the softmax variable, indicating that this computes the value wrt the y'th column of the weight matrix. Our goal of computing the gradient of the loss wrt the weights we want to optimize ($ \frac{\delta L}{\delta w} $), is split into the product of the gradient of the loss wrt the softmax ($ \frac{\delta L}{\delta S_{y}} $) times the gradient of the softmax wrt the weights ($ \frac{\delta S_{y}}{\delta w} $). </p><p>$$ \frac{\delta S_{y}}{\delta w}=\frac{e^{X_{n}w_{y}}}{\sum_{c=1}^{C}e^{X_{n}w_{c}}}\frac{1}{\delta w} $$</p><p>$$ \frac{\delta L}{\delta S_{y}}=-\frac{1}{N}\sum_{n=1}^{N}log(S_{y})\frac{1}{\delta S_{y}} $$</p><h3 id="and-conquer-the-softmax">... and conquer (the softmax)</h3><p>Let us assume for a second that we have only 3 classes  ($ C = 3 $) that we want to classify. Furthermore, we will just look at the first sample in the dataset. In this case, we can write out the summand in the denominator so the whole expression looks a lot less intimidating to differentiate.</p><p>$$ \frac{\delta S_{y}}{\delta w}=\frac{e^{X_{1}w_{y}}}{e^{X_{1}w_{1}}+e^{X_{1}w_{2}}+e^{X_{1}w_{3}}}\frac{1}{\delta w} $$</p><p>The question that now arises is, what is the ground truth ($ y $) for our first sample? If our MCC problem deals with identifying cats ($ y = 1 $), dogs ($ y = 2 $), and chickens ($ y = 3 $) on each sample (image) we can just assume that in fact on the first sample ($ n = 1 $) a dog can be seen. Thus, our ground truth for the first image is ($ y = 2 $). Here are the first four images of our dataset with the corresponding ground truth underneath them.</p><!--kg-card-begin: html--><table style="padding: 10px">
    <tr align="center">
        <td>($ n = 1 $)</td>
        <td>($ n = 2 $)</td>
        <td>($ n = 3 $)</td>
        <td>($ n = 4 $)</td>
    </tr>
    <tr style="margin: 10px">
        <td><img src="https://github.com/frank690/sffresch-code/raw/main/differentiate-all-the-things/images/dog.png"></img></td>
		<td><img src="https://github.com/frank690/sffresch-code/raw/main/differentiate-all-the-things/images/cat.png"></img></td>
		<td><img src="https://github.com/frank690/sffresch-code/raw/main/differentiate-all-the-things/images/chick.png"></img></td>
		<td><img src="https://github.com/frank690/sffresch-code/raw/main/differentiate-all-the-things/images/cat2.png"></img></td>
</tr>
    <tr align="center">
        <td>($ y = 2 $)</td>
        <td>($ y = 1 $)</td>
        <td>($ y = 3 $)</td>
        <td>($ y = 1 $)</td>
    </tr>
</table><!--kg-card-end: html--><p>Since we have three different columns in the weight matrix ($ w_{1}, w_{2}, w_{3} $) we also need to do three separate derivatives (for now). Luckily for us, nothing fancy is happening when doing the actual differentiation, so all we need to know is a little calculus. More specifically, the <a href="https://en.wikipedia.org/wiki/Quotient_rule">quotient rule</a> as well as <a href="https://www.google.com/search?q=how+to+differentiate+an+exponential+function">how to differentiate an exponential function</a>. Starting the differentiation wrt our first weight column ($ w_{1} $) ...</p><p>$$ \frac{\delta S_{2}}{\delta w_{1}}=\frac{e^{X_{1}w_{2}}}{e^{X_{1}w_{1}}+e^{X_{1}w_{2}}+e^{X_{1}w_{3}}}\frac{1}{\delta w_{1}} $$</p><p>With the quotient-rule in mind we can define $ g(w_{1}) = e^{X_{1}w_{2}} $ and $ h(w_{1}) = e^{X_{1}w_{1}}+e^{X_{1}w_{2}}+e^{X_{1}w_{3}} $. Differentiating these terms brings us to $ g'(w_{1}) = 0 $ (since there is no $ w_{1} $ in the equation) and $ h'(w_{1}) = X_{1}^{T} e^{X_{1}w_{1}} $. Putting it all together we therefore get</p><p>$$ \frac{\delta S_{2}}{\delta w_{1}}=\frac{0*(e^{X_{1}w_{1}}+e^{X_{1}w_{2}}+e^{X_{1}w_{3}}) - e^{X_{1}w_{2}}*X_{1}^{T}e^{X_{1}w_{1}}<br>}{(e^{X_{1}w_{1}}+e^{X_{1}w_{2}}+e^{X_{1}w_{3}})^{2}} =\frac{-X_{1}^{T}*e^{X_{1}w_{2}}*e^{X_{1}w_{1}}<br>}{(e^{X_{1}w_{1}}+e^{X_{1}w_{2}}+e^{X_{1}w_{3}})^{2}} $$</p><p>Due to matrix calculus (and the fact that we always want our derivatives to have the same shape as what we differentiate after), $ X_{1}^{T} $ was transposed. At this point, we can take a hard look at the equation and see that the original softmax function can be found in its differentiation. This becomes more apparent when we re-organize the fraction a bit. Since $ w_{3} $ is not in the nominator of our original differentiation, it behaves just like $ w_{1} $ and we can easily write down its final derivative as well.</p><p>$$ \frac{\delta S_{2}}{\delta w_{1}} = -X_{1}^{T} * \frac{e^{X_{1}w_{2}}}{(e^{X_{1}w_{1}}+e^{X_{1}w_{2}}+e^{X_{1}w_{3}})} * \frac{e^{X_{1}w_{1}}}{(e^{X_{1}w_{1}}+e^{X_{1}w_{2}}+e^{X_{1}w_{3}})} = -X_{1}^{T} * S_{2} * S_{1} $$</p><p>$$ \frac{\delta S_{2}}{\delta w_{3}} = -X_{1}^{T} * S_{2} * S_{3} $$</p><p>As you might have guessed by now, we can not do the same with $ w_{2} $ since it is our ground truth, due to the fact that image one ($ n = 1 $) shows a dog ($ y = 2 $). Therefore the applied quotient rule looks a little bit different in the nominator, compared to what we did before.</p><p>$$ \frac{\delta S_{2}}{\delta w_{2}} = \frac{e^{X_{1}w_{2}}}{e^{X_{1}w_{1}}+e^{X_{1}w_{2}}+e^{X_{1}w_{3}}}\frac{1}{\delta w_{2}} = \frac{X_{1}^{T}e^{X_{1}w_{2}}*(e^{X_{1}w_{1}}+e^{X_{1}w_{2}}+e^{X_{1}w_{3}})-e^{X_{1}w_{2}}*X_{1}^{T}e^{X_{1}w_{2}}}{(e^{X_{1}w_{1}}+e^{X_{1}w_{2}}+e^{X_{1}w_{3}})^{2}} $$</p><p>$$ = X_{1}^{T}*<br>\frac{e^{X_{1}w_{2}}}{e^{X_{1}w_{1}}+e^{X_{1}w_{2}}+e^{X_{1}w_{3}}} -<br>X_{1}^{T}*<br>\frac{e^{X_{1}w_{2}}}{e^{X_{1}w_{1}}+e^{X_{1}w_{2}}+e^{X_{1}w_{3}}} *<br>\frac{e^{X_{1}w_{2}}}{e^{X_{1}w_{1}}+e^{X_{1}w_{2}}+e^{X_{1}w_{3}}} = X_{1}^{T}*S_{2} * (1 - S_{2})$$</p><p>Finally, we can see that there are two different solutions for the differentiation of the softmax function wrt the weights. This difference depends ultimately on the question of whether the differentiation is wrt the current ground truth ($ y $) or not. Let us introduce the variable $ j $ to indicate labels that are <strong>not</strong> the ground truth ($ j \neq y $). In our example regarding the first image we thus have $ y = 2 $ and $ j = 1, 3 $. With this new variable, we can generalize our previously computed derivatives. And since these equations hold not only regarding the first but all input images, we can put $ n $ back into place.</p><p>$$ \frac{\delta S_{y}}{\delta w_{j}} = -X_{n}^{T} * S_{y} * S_{j} $$</p><p>$$ \frac{\delta S_{y}}{\delta w_{y}} = X_{n}^{T}*S_{y} * (1 - S_{y}) $$</p><h3 id="and-conquer-the-log-loss">... and conquer (the log-loss)</h3><p>Differentiating the log-loss is a lot easier compared to the softmax function. All you need to know here is <a href="https://www.google.com/search?q=how+to+differentiate+a+logarithm">how to differentiate the logarithm</a> and the <a href="https://en.wikipedia.org/wiki/Chain_rule">chain rule</a>. Furthermore please do not panic when you see the summation sign ($ \sum $) in the equation, it just wants to play. Also, we do not have to distinguish between $ S_{y} $ and $ S_{j} $ since there is only one $ S $ in the equation. Thus, the differentiation is valid for both cases.</p><p>$$ \frac{\delta L}{\delta S}=-\frac{1}{N}\sum_{n=1}^{N}log(S)\frac{1}{\delta S} = -\frac{1}{N}\sum_{n=1}^{N} \frac{1}{S} = -\frac{1}{N} * (\frac{1}{S} + ... + \frac{1}{S}) = -\frac{1}{N} * N * \frac{1}{S} = -\frac{1}{S}$$</p><h3 id="putting-it-all-together">Putting it all together</h3><p>As discussed before we were able to split the softmax and log-loss differentiation due to calculus being our friend. Looking now at both cases ($ j = y $) and ($ j \neq y $) we can easily compute the final solution to our initial differentiation problem.</p><p>$$ \frac{\delta L}{\delta w_{y}} = \frac{\delta L}{\delta S_{y}} * \frac{\delta S_{y}}{\delta w_{y}} = -\frac{1}{S_{y}} * X_{n}^{T} * S_{y} * (1 - S_{y}) = - X_{n}^{T} * (1 - S_{y}) $$</p><p>$$ \frac{\delta L}{\delta w_{j}} = \frac{\delta L}{\delta S_{j}} * \frac{\delta S_{j}}{\delta w_{j}} = -\frac{1}{S_{j}} * (-X_{n}^{T}) * S_{y} * S_{j} = X_{n}^{T} * S_{j} $$</p><h3 id="code-over-all">Code over all</h3><!--kg-card-begin: html--><pre class="line-numbers language-python"><code>import numpy as np
from typing import Tuple

def softmax_loss_log(W: np.ndarray, X: np.ndarray, y: np.ndarray) -> Tuple[float, np.ndarray]:
    """
    A loss function consisting of a softmax layer that is fed into the log-loss.
    Loss as well as the weight derivatives are returned.
    :param W: Weight matrix of shape (D x C)
    :param X: Data matrix of shape (N X D)
    :param y: Vector of labels (N x 1)
    :return: Tuple of loss value and weight derivatives of shape (D x C).
    """
    N, _ = X.shape
    
    y_prediction = X @ W  # (N x C)
    y_prediction -= np.max(y_prediction, axis=1, keepdims=True)
    
    S_nominator = np.exp(y_prediction)
    S_denominator = np.exp(y_prediction).sum(axis=1, keepdims=True)
    S = S_nominator / S_denominator

    loss = -(1/N) * np.sum(
        np.log(
            S[np.arange(N), y]
        )
    )

    S[np.arange(N), y] -= 1
    dW = (X.T @ S) / N

    return loss, dW
</code></pre><!--kg-card-end: html--><h3 id="update-code-implementation-details">Update: Code implementation details</h3><p>Most of the implementation is relatively straightforward, doing exactly what we described previously, like computing the prediction of our ANN (line 15), the softmax matrix (18-20), or the loss (line 22-26). But there are also a few tricks in place I will explain a bit more in detail.</p><h4 id="numerical-stability">Numerical Stability</h4><pre><code class="language-python">y_prediction -= np.max(y_prediction, axis=1, keepdims=True)</code></pre><p>In line 16 we subtract the maximum value of the prediction (<strong>row-wise</strong>). Remember that the prediction has the dimension $(N \times C)$, and we did not apply softmax to it, yet. Thus, the values here can be (literally) anything. By subtracting the maximum of each row, we make sure that the biggest value is always 0. Thus we moved the range of possible values to $(-\infty, 0]$. As a result, when we apply the softmax algorithm next, our denominator (line 19) is guaranteed to be at least 1, so we will not run into issues like dividing by zero. For more details <a href="https://stackoverflow.com/a/42606665">click here</a>.</p><h4 id="softmax-values-for-j-y">Softmax values for (j = y)</h4><pre><code class="language-python">S[np.arange(N), y] -= 1
dW = (X.T @ S) / N</code></pre><p>In lines 28-29 we subtract all values where our prediction is the correct class, by 1. Afterward, we compute the derivatives wrt the weight matrix. If you look at our previously derived formulae (just above the code block) for the loss wrt the weights ($\frac{\delta L}{\delta w_{j}}$), you can see that this is exactly what we are doing in the code (ignore the division by N, for now). But as we know, $\frac{\delta L}{\delta w_{y}}$ (the loss wrt the weights of the <strong>correct</strong> prediction class) needs to be treated differently. To adjust for this difference, we do the subtraction in line 28.</p><p>$$ \frac{\delta L}{\delta w_{j}} =  X_{n}^{T} * S_{j} =  X_{n}^{T} * (S_{y} - 1) = X_{n}^{T} * S_{y} - X_{n}^{T} = - X_{n}^{T} * (1 - S_{y}) = \frac{\delta L}{\delta w_{y}} $$</p><h4 id="division-by-n">Division by N</h4><pre><code class="language-python">dW = (X.T @ S) / N</code></pre><p>In line 29 we also divide by N (the number of samples). If you look at the dimensions of the multiplication of $X^T * S$, you see that it is $(D \times N) * (N \times C) = (D \times C)$. We can always do this multiplication, independent of the number of samples (the size of N). But does this mean, that N does not influence the values of dW? The answer is no. Surely the values in dW will increase when the number of samples will increase. To counter this, we divide by N.</p><h3 id="synonyms">Synonyms</h3><p>MCC - multiclass classification<br>wrt - with respect to<br></p><p></p><h3 id><br></h3>
                                </div>
                                    <h2 class="post-tags-header">Tags</h2>
    <div class="post-tags">
            <a class="post-tag" id="ANN" href="../tag/ann/index.html" style="background-color: #8e0101;">ANN</a>
            <a class="post-tag" id="theory" href="../tag/theory/index.html" style="background-color: #bf7e0d;">theory</a>
            <a class="post-tag" id="data-science" href="../tag/data-science/index.html" style="background-color: #366356;">data-science</a>
            <a class="post-tag" id="softmax" href="../tag/softmax/index.html" style="background-color: #1c226d;">softmax</a>
            <a class="post-tag" id="log-loss" href="../tag/log-loss/index.html" style="background-color: #ff2424;">log-loss</a>
            <a class="post-tag" id="classification" href="../tag/classification/index.html" style="background-color: #517510;">classification</a>
    </div>
                            </div>
                        </article>
                </main>
            </div>
        </div>
    </div>
</div>
        </div>
        <footer class="site-footer">
    <div class="container">
        <div class="footer-inner">
            <div class="social">
                <a class="social-item social-item-rss"
                    href="https://feedly.com/i/subscription/feed/http://frank690.github.io/rss/"
                    target="_blank" rel="noopener noreferrer" aria-label="RSS">
                    <i class="icon icon-rss"></i>
                </a>
            </div>
            <div class="copyright">
                Powered by <a href="https://ghost.org/" target="_blank">Ghost</a>
            </div>
        </div>
    </div>
</footer>    </div>


    <div class="dimmer"></div>
<div class="off-canvas">
    <div class="burger burger-close hidden-lg hidden-xl">
    <div class="burger-bar"></div>
    <div class="burger-bar"></div>
</div>    <div class="mobile-menu">
        <ul class="nav-list u-plain-list">
        <li
            class="menu-item menu-item-whoami">
            <a class="menu-item-link" href="../whoami/index.html">whoami</a>
        </li>
        <li
            class="menu-item menu-item-projects">
            <a class="menu-item-link" href="../projects.html">projects</a>
        </li>
</ul>
    </div>
    <aside class="widget-area">
    <div class="widget widget-facebook widget-no-title u-shadow">
    <div class="fb-page" data-href="__YOUR_FACEBOOK_PAGE_URL__"
        data-small-header="false" data-hide-cover="false"
        data-show-facepile="true" data-hide-cta="false" data-tabs="none">
    </div>
</div>    <div class="widget widget-tags">
    <h4 class="widget-title">Tags</h4>
        <div class="tag-feed">
                <a class="post-tag" id="ANN" href="../tag/ann/index.html" style="background-color: #8e0101;">ANN</a>
                <a class="post-tag" id="DIY" href="../tag/diy/index.html" style="background-color: #a87d34;">DIY</a>
                <a class="post-tag" id="classification" href="../tag/classification/index.html" style="background-color: #517510;">classification</a>
                <a class="post-tag" id="data-science" href="../tag/data-science/index.html" style="background-color: #366356;">data-science</a>
                <a class="post-tag" id="log-loss" href="../tag/log-loss/index.html" style="background-color: #ff2424;">log-loss</a>
                <a class="post-tag" id="machine-learning" href="../tag/machine-learning/index.html" style="background-color: #001999;">machine-learning</a>
                <a class="post-tag" id="neural-network" href="../tag/neural-network/index.html" style="background-color: #600094;">neural-network</a>
                <a class="post-tag" id="python" href="../tag/python/index.html" style="background-color: #026600;">python</a>
                <a class="post-tag" id="softmax" href="../tag/softmax/index.html" style="background-color: #1c226d;">softmax</a>
                <a class="post-tag" id="theory" href="../tag/theory/index.html" style="background-color: #bf7e0d;">theory</a>
        </div>
</div></aside></div>
    <script src="https://code.jquery.com/jquery-3.3.1.min.js"
        integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
        crossorigin="anonymous">
        </script>
    <script src="../assets/built/main.min.js%3Fv=a3ac031e72"></script>

    <div id="fb-root"></div>
<script async defer crossorigin="anonymous"
    src="https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v3.2"></script>
    

    <!-- copy to clipboard -->
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/prism.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/plugins/toolbar/prism-toolbar.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>

<!-- code styling -->
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.6.0/components/prism-python.min.js" ></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.6.0/plugins/line-numbers/prism-line-numbers.min.js" ></script>

<!-- cookie consent -->
<script src="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.js" data-cfasync="false"></script>
<script>
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#ffffff"
    },
    "button": {
      "background": "#8ec760",
      "text": "#ffffff"
    }
  },
  "theme": "edgeless"
});
</script>
</body>

</html>